<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <title>William Grimes: Introduction to Statistical Learning</title>
        
<meta name="keywords" content="bash,illegal fishing,whitepaper,text mining,project,statistics,NLP,bitcoin,notes,python,machine learning,kaggle,unix,computer vision,segmentation">

<meta name="description" content="An Introduction to Statistical Learning, notes... ">

<meta property="og:description" content="An Introduction to Statistical Learning, notes... ">

<meta property="og:url" content="http://williamgrimes.xyz/posts/2020-05-12-isl-notes/" />
<meta property="og:title" content="Introduction to Statistical Learning" />
<meta property="og:type" content="article" />

        <meta content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no" name="viewport">
        <link rel="icon" href="/img/headshot.png">
        <link type="text/css" href="/fonts/sample.css" rel="stylesheet">
        <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
        <link href="/css/normalize.css" rel="stylesheet" type="text/css" />
        <link href="/css/blog.css" rel="stylesheet" type="text/css" />
        <link href="/css/lotus-highlightjs.min.css" rel="stylesheet" type="text/css" />
    </head>
    <body>
        <aside id="sidebar">
            <div id="logo">
                <a title="William Grimes" href="/">
                    <div class="text desktop-and-tablet-only">William Grimes</div>
                    <div class="text mobile-only">William Grimes</div>
                    <img class="icon" src="/img/headshot.png">
                </a>
            </div>

            <nav id="sidebar-links">
                <nav id="menucont">
                    <input type="checkbox" id="menu-checkbox" class="menu-checkbox">
                    <ul class="menu">
                        <li ><a title="Home" href="/"><div class="menu-item-text">Home</div></a></li>
                        <li ><a title="Archives" href="/archives/"><div class="menu-item-text">Archives</div></a></li>
                        
                        <li ><a title="Tags" href="/tags/"><div class="menu-item-text">Tags</div></a></li>
                        
                        
                        <li >
                            <a href="/pages/cv/"><div class="menu-item-text">Curriculum Vitae</div></a>
                        </li>
                        
                        <!-- <li><a title="RSS" href="/feed.xml"><div class="menu-item-text">RSS</div></a></li> -->
                    </ul>

                <nav id="socialmedia">
                    <ul>
                      <li><a title="GitHub" href="https://github.com/williamgrimes/" rel="external"><span class="fa fa-github fa-2x" style="color:#ECEFF4"></span></a></li>
                      <li><a title="Stack Overflow" href="https://stackoverflow.com/users/2545164/william-grimes/" rel="external"><span class="fa fa-stack-overflow fa-2x" style="color:#ECEFF4"></span></a></li>
                      <li><a title="LinkedIn" href="https://www.linkedin.com/in/william-grimes/" rel="external"><span class="fa fa-linkedin fa-2x" style="color:#ECEFF4"></span></a></li>
                    </ul>
                </nav>
            </nav>
        </aside>

        <article id="main">
            
<div id="post">
    <div class="post-header">
    <div id="post-meta">
        <h1>Introduction to Statistical Learning</h1>
        <div class="byline">
            <span class="date">12 May 2020</span>
            
        </div>
    </div>
</div>
<div>
    <ol class="toc"><li><a href="#statistical-learning">Statistical Learning:</a></li><li><a href="#linear-regression">Linear Regression:</a></li><li><a href="#classification">Classification:</a></li><li><a href="#resampling-methods">Resampling methods:</a></li><li><a href="#linear-model-selection-and-regularization">Linear Model Selection and Regularization:</a></li><li><a href="#moving-beyond-linearity">Moving Beyond Linearity:</a></li><li><a href="#tree-based-methods">Tree-Based Methods:</a></li><li><a href="#support-vector-machines">Support Vector Machines:</a></li><li><a href="#unsupervised-learning">Unsupervised Learning:</a></li></ol>
    <p><em>An Introduction to Statistical Learning, notes... </em></p><p><b>Motivation</b>: some notes on reviewing an Introduction to Statistical Learning (ISL) by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. The book is freely available <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/" target="_blank">here</a>. `</p><h2 id="statistical-learning">Statistical Learning:</h2><p>Statistical learning refers to a set of tools for understanding data and recognising patterns. In the case of <em>supervised</em> learning the goal is to predict an output or make a decision without exlicit rule based programming but to use labeled input data to learn patterns. In contrast, <em>unsupervised</em> learning attempts to find patterns in data without pre-existing labels.</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=Y = f(X) %2B \epsilon" /></p><p>There is a relationship between a response <img src="https://render.githubusercontent.com/render/math?math=Y" /> and a set of <img src="https://render.githubusercontent.com/render/math?math=X" /> predictors <img src="https://render.githubusercontent.com/render/math?math=(x_1, x_2, x_3, ..., x_p)" />, with <img src="https://render.githubusercontent.com/render/math?math=\epsilon" /> being a random error term with a mean of zero. Statistical learning can be used to estimate the mapping between the response (dependent variable) and the predictors.</p><p>Statistical learning aims to best approximate the mapping function <img src="https://render.githubusercontent.com/render/math?math=f" /> between the response (dependent variable) and the predictors. In general, an estimation of <img src="https://render.githubusercontent.com/render/math?math=f" />, denoted by <img src="https://render.githubusercontent.com/render/math?math=\hat{f}" />, will not be perfect and will introduce error. The error between <img src="https://render.githubusercontent.com/render/math?math=f" /> and <img src="https://render.githubusercontent.com/render/math?math=\hat{f}" /> has a <em>reducible error</em> component that is reduced by better approximating the mapping function <img src="https://render.githubusercontent.com/render/math?math=\hat{f}" />, and an irreducible component due to the error term <img src="https://render.githubusercontent.com/render/math?math=\epsilon" />.</p><p>The two main motivations for learning this mapping are <em>prediction</em> and <em>inference</em>, where:</p><ul><li><p><strong>Prediction</strong>, in the case where the inputs <img src="https://render.githubusercontent.com/render/math?math=X" /> are available, and since the error term averages to zero we predict as <img src="https://render.githubusercontent.com/render/math?math=\hat{Y} = \hat{f}(X)" />, in many cases the exact form of <img src="https://render.githubusercontent.com/render/math?math=\hat{f}" /> is not of conern.</p></li><li><p><strong>Inference</strong>, is used to understand the way in which <img src="https://render.githubusercontent.com/render/math?math=Y" /> is affected as <img src="https://render.githubusercontent.com/render/math?math=X_1, X_2, ... X_p" /> change. To answer questions such as: which predictors are associated with the response, or what is th relationship between the response and each predictor?</p></li></ul><p>Approaches to estimate <img src="https://render.githubusercontent.com/render/math?math=f" /> share certain characteristics and can broadly be deemed <em>parametric</em> or <em>non-parametric</em>:</p><ul><li><p><strong>parametric</strong> - make an assumption about the functional nature, or shape, of <img src="https://render.githubusercontent.com/render/math?math=f" />. For example, assume that <img src="https://render.githubusercontent.com/render/math?math=f" /> is linear, yielding a linear model. Then estimate the set of parameters. In general, it is much simpler to estimate a set of parameters than it is to estimate an entirely arbitrary function <img src="https://render.githubusercontent.com/render/math?math=f" />. A disadvantage of this approach is that the specified model won’t usually match the true form of <img src="https://render.githubusercontent.com/render/math?math=f" />.</p></li><li><p><strong>non-parametric</strong> methods do not make explicit assumptions about the functional form of <img src="https://render.githubusercontent.com/render/math?math=f" />. They are more flexible and can fit to a wider range of forms of <img src="https://render.githubusercontent.com/render/math?math=f" />. The downside of this is that they often require many parameters to fit to <img src="https://render.githubusercontent.com/render/math?math=f" />.</p></li></ul><p><em>supervised</em> or <em>unsupervised</em>, where:</p><ul><li><p><strong>supervised learning</strong> refers to those scenarios in which for each observation of the predictor measurements there is an associated response measurement. The model generated relates the predictors to the response with the goal of accurately predicting future observations or of better inferring the relationship between the predictors and the response.</p></li><li><p><strong>unsupervised learning</strong> refers to those scenarios in which for each observation of the predictor measurements, there is no associated response. There is no response variable that can supervise the analysis that goes into generating a model. Often unsupervised learning involves <em>cluster analysis</em>, a process by which observations are arranged into relatively distinct groups.</p></li></ul><p><em>regression</em> or <em>classification</em>, where:</p><ul><li><p><strong>regression</strong> problems have output variables that take on quantitative continuous values.</p></li><li><p><strong>classification</strong> problems have outputs that whose values are distinct classes or categories. Problems with a qualitative response are often referred to as classification problems.</p></li></ul><p><strong>Quality of fit</strong> is a way to asses the performance of a model used to solve a regression problem determining how well predictions match the observed data. A typical metric for this is the mean squared error (MSE), given as,</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{f}(x_i))^2" /></p><p>where <img src="https://render.githubusercontent.com/render/math?math=\hat{f}(x_i)" /> is the prediction given on the <img src="https://render.githubusercontent.com/render/math?math=i" />th observation.  We would like to choose a model that has as low a mean-squared error on the test group as possible.</p><p>The <strong>flexibility</strong> or number of <strong>degrees of freedom</strong> of a model quantifies the number of parameters in the model that are free to vary. The degrees offreedom is a quality that summarizes the flexibility of a curve. As the number of degrees of freedom of a model increases the training MSE will usually decrease, whilst the test MSE may not. When the training MSE is small but the test MSE is large, the model is described as overfitting the data. Almost always the training MSE will be less than the test MSE because most methods aim to minimize the training MSE.</p><p>The expected test MSE for a given value <img src="https://render.githubusercontent.com/render/math?math=x_0" /> can be decomposed into the sum of three quantities. The variance of <img src="https://render.githubusercontent.com/render/math?math=\hat{f}(x_0)" />, the squared bias of <img src="https://render.githubusercontent.com/render/math?math=\hat{f}(x_0)" />, and the variance of the error term, <img src="https://render.githubusercontent.com/render/math?math=\epsilon" />. Formally, the balance of these opposing forces known as the <strong>bias-variance tradeoff</strong> can be written as follows:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=E (y_0 - \hat{f}(x_0))^2 = Var(\hat{f}(x_0)) %2B [Bias(\hat{f}(x_0))]^2  %2B  Var(\epsilon)" /></p><p>to minimise the expected test error, a method should minimise both the variance and the bias. It can be seen that the expected test mean squared error can never be less than <img src="https://render.githubusercontent.com/render/math?math=Var(\epsilon)" /> the irreducible error. Variance refers to the amount by which <img src="https://render.githubusercontent.com/render/math?math=\hat{f}" /> would change if it were estimated using a different training data set. In general, more flexible methods have higher variance, bias refers to the error that is introduced by approximating a potentially complex function using a simple model. More flexible models tend to have less bias. In general, the more flexible the statistical learning method, the more variance will increase and bias decrease.</p><p>In the classification setting the quality of fit can be assessed looking at the training error. The training error rate is the proportion of errors that are made when applying <img src="https://render.githubusercontent.com/render/math?math=\hat{f}" /> to the training observations. Formally stated as,</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\frac{1}{n}\sum_{i=1}^{n}I(y_i \neq \hat{y}_i)" /></p><p>where <img src="https://render.githubusercontent.com/render/math?math=I" /> is an indicator variable that equals 0 when <img src="https://render.githubusercontent.com/render/math?math=y = \hat{y}" /> and equals 1 when <img src="https://render.githubusercontent.com/render/math?math=y \neq \hat{y}" />. In simple terms, the error rate is the ratio of incorrect classifications to the observation count. As in the regression scenario, a good classifier is one for which the test error rate is smallest.</p><p>The <strong>Bayes classifier</strong> is a simple classifier that assigns each observation the most likely class, which on average minimises the test error rate given its predictor variables. In Bayesian terms, a test observation should be classified for the predictor vector <img src="https://render.githubusercontent.com/render/math?math=x_0" /> to the class <img src="https://render.githubusercontent.com/render/math?math=j" /> for which,</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=Pr(Y=j|X=x_0)" /></p><p>is largest. That is, the class for which the conditional probability that <img src="https://render.githubusercontent.com/render/math?math=Y=j" />
, given the observed predictor vector <img src="https://render.githubusercontent.com/render/math?math=x_0" />, is largest. This classifier is called the Bayes classifier.</p><p>In a two-class scenario, this can be stated as  <img src="https://render.githubusercontent.com/render/math?math=Pr(Y=1|X=x_0) &gt; 0.5" /> matching class A when true and class B when false. The threshold where the classification probability is exactly 50% is known as the Bayes decision boundary.</p><p>The Bayes classifier yields the lowest possible test error rate since it will always choose the class with the highest probability. The Bayes error rate can be stated formally as <img src="https://render.githubusercontent.com/render/math?math=1 - E(maxPr_j(Y=j|X)" />. The Bayes error rate can also be described as the ratio of observations that lie on the "wrong" side of the decision boundary. Unfortunately, the conditional distribution of Y given X is often unknown, so the Bayes classifier is most often unattainable.</p><p>Many modelling techniques try to compute the conditional distribution of <img src="https://render.githubusercontent.com/render/math?math=Y" /> given
<img src="https://render.githubusercontent.com/render/math?math=X" /> and then provide estimated classifications based on the highest estimated probability. The <strong>k-nearest neighbors classifier</strong> is one such method.</p><p>The K-nearest neighbors classifier takes a positive integer K and first identifies the K points that are nearest to <img src="https://render.githubusercontent.com/render/math?math=x_0" />, represented by <img src="https://render.githubusercontent.com/render/math?math=N_0" />. It next estimates the conditional probability for class <img src="https://render.githubusercontent.com/render/math?math=j" /> based on the fraction of points in <img src="https://render.githubusercontent.com/render/math?math=N_0" /> who have a response equal to <img src="https://render.githubusercontent.com/render/math?math=j" />. Formally, the estimated conditional probability can be stated as</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=Pr(Y=j|X=x_0)= \frac{1}{K}\sum_{i{\epsilon}N_0}I(y_i=j)" /></p><p>The K-Nearest Neighbor classifier then applies Bayes theorem and yields the classification with the highest probability. Despite its simplicity, the K-Nearest Neighbor classifier often yields results that are surprisingly close to the optimal Bayes classifier. The choice of K can have a drastic effect on the yielded classifier. Too low a K yields a classifier that is too flexible, has too high a variance, and low bias.</p><p>Conversely, as K increases, the yielded classifier becomes less flexible, with a low variance, but high bias. In both regression and classification scenarios, choosing the correct level of flexibility is critical to the success of the model.</p><p><b>Further reading:</b></p><ul><li><a href="https://youtu.be/C3nIFH649wY" target="_blank"> Bias-Variance decomposition.</a></li></ul><h2 id="linear-regression">Linear Regression:</h2><p><em>Simple linear regression</em> predicts a quantitative response  <img src="https://render.githubusercontent.com/render/math?math=Y" /> from a single predictor variable <img src="https://render.githubusercontent.com/render/math?math=X" />, assuming they are linearly related, formally:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=Y= \beta_0  %2B \beta_1X" /></p><p>where
<img src="https://render.githubusercontent.com/render/math?math=\beta_{0}" /> and <img src="https://render.githubusercontent.com/render/math?math=\beta_1" />, are two unknown constants (parameters) representing the <em>intercept</em> and <em>slope</em> of the linear model. In practice these parameters are unknown and the goal of modelling with a linear regression is to find values of <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta_0}" /> and <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta_1}" /> that fit the data well. The most common approach to measure the <em>closeness</em> of the line to the data is using the <em>least squares</em> criterion. Assuming that for <img src="https://render.githubusercontent.com/render/math?math=i" />:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\hat{Y}_i = \hat{\beta}_0  %2B \hat{\beta}_1X" /></p><p>then the <img src="https://render.githubusercontent.com/render/math?math=i" />th residual can be represented as the <strong>residual sum of squares</strong>:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=RSS = e^2_1 %2B e^2_2 %2B ... %2B e^2_n" />
    or  
    <img src="https://render.githubusercontent.com/render/math?math=RSS = (y_1-\hat{\beta}_0-\hat{\beta}_1x_1)^2 %2B (y_2-\hat{\beta}_0-\hat{\beta}_1x_2)^2 %2B ... %2B (y_n-\hat{\beta}_0-\hat{\beta}_1x_n)^2" /></p><p>assuming sample means of <img src="https://render.githubusercontent.com/render/math?math=\bar{y} = \frac{1}{n}\sum_{i=1}^{n}y_i" />, and <img src="https://render.githubusercontent.com/render/math?math=\bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_i" />.</p><p>calculus can be used (see further reading) to estimate the least squares coefficient estimates for linear regression minimising the residual sum of squares:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\beta_1 = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{(x_i - \bar{x})^2}" />
    and
    <img src="https://render.githubusercontent.com/render/math?math=\beta_0 = \bar{y} - \hat{\beta_1} \bar{x}" /></p><p><strong>How to assess coefficient estimate accuracy?</strong> The model used by simple linear regression defines the population regression line, which describes the best linear approximation to the true relationship between <img src="https://render.githubusercontent.com/render/math?math=X" /> and <img src="https://render.githubusercontent.com/render/math?math=Y" /> for the population. A simple linear regression represents the relationship as:</p><img src="https://render.githubusercontent.com/render/math?math=Y= \beta_0 %2B \beta_1X  %2B \epsilon" /><p>where <img src="https://render.githubusercontent.com/render/math?math=\epsilon" /> is the <em>irreducible error</em>, which is a catchall error term for what is missed by the simple model given that the relationship is probably not linear, and their maybe other input variables affecting the response, this term is assumed to be independent of <img src="https://render.githubusercontent.com/render/math?math=X" />.</p><p>The coefficient estimates yielded by least squares regression characterize the least squares line,</p><img src="https://render.githubusercontent.com/render/math?math=\hat{Y}= \hat{\beta_0} %2B \hat{\beta_1}X_i" /><p>The difference between the population regression line and the least squares line is similar to the difference that emerges when using a sample to estimate the characteristics of a population.</p><p>In linear regression, the unknown coefficients, <img src="https://render.githubusercontent.com/render/math?math=\beta_0" /> and <img src="https://render.githubusercontent.com/render/math?math=\beta_1" /> define the population regression line, whereas the estimates of those coefficients, <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta_0}" /> and <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta_1}" /> define the least squares line.</p><p>Parameter estimates for a sample may overestimate or underestimate the value of a particular parameter, but an unbiased estimator does not systemically overestimate or underestimate the true parameter. This means that using an unbiased estimator and a large number of data sets, the values of the coefficients <img src="https://render.githubusercontent.com/render/math?math=\beta_0" /> and <img src="https://render.githubusercontent.com/render/math?math=\beta_1" /> could be determined by averaging the coefficient estimates from each of those data sets.</p><p>The standard error can be used to estimate the accuracy of a single estimated value, such as an average.</p><img src="https://render.githubusercontent.com/render/math?math=Var(\hat{\mu}) = SE(\hat{\mu})^2 = \frac{\sigma^2}{n}" /><p>where <img src="https://render.githubusercontent.com/render/math?math=\mu" />, is the standard deviation of each <img src="https://render.githubusercontent.com/render/math?math=y_i" />. The standard error describe approximately the amount that <img src="https://render.githubusercontent.com/render/math?math=\hat{\mu}" /> differs from <img src="https://render.githubusercontent.com/render/math?math=\mu" />. More observations gives a larger denominator n and a smaller standard error.</p><p>Standard errors for <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta_0}" /> and <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta_1}" /> are calculated as:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=SE(\beta_0)^2 = \sigma^2 [ \frac{1}{n} %2B \frac{\bar{x}^2}{\sum{n}{i=1}(x_i - \bar{x})^2} ] " /></p>
and,
<p align="center"><img src="https://render.githubusercontent.com/render/math?math=SE(\beta_1)^2 = \frac{\sigma^2}{\sum_{n}^{i=1}(x_i - \bar{x})^2} " /></p><p>where</p><p><img src="https://render.githubusercontent.com/render/math?math=\sigma^2 = Var(\epsilon)" /> and <img src="https://render.githubusercontent.com/render/math?math=\epsilon_i" /> is not correlated with <img src="https://render.githubusercontent.com/render/math?math=\sigma^2" />,</p><p>which is usually not known but can be estimated from the data, known as the <em>residual standard error</em> (RSE):</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=RSE = \sqrt{\frac{RSS}{(n - 2)}}" /></p><p>where RSS is the residual sum of squares. Standard errors can be used to compute confidence intervals and prediction intervals. A confidence interval is defined as a range of values such that there’s a certain likelihood that the range will contain the true unknown value of the parameter. For simple linear regression the 95% confidence interval for <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta_1}" /> can be approximated by: <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta_1} \pm 2 \times SE(\hat{\beta_1})" />. Similarly, for <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta_0}" />, <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta_0} \pm 2 \times SE(\hat{\beta_0})" />.</p><p>The accuracy of an estimated prediction depends on whether we wish to predict an individual response or the average response. When predicting an individual response, <img src="https://render.githubusercontent.com/render/math?math=y = f(x) %2B \epsilon" />, a prediction interval is used. When predicting an average response, <img src="https://render.githubusercontent.com/render/math?math=f(x)" /> a confidence interval is used.</p><p>Prediction intervals will always be wider than confidence intervals because they take into account the uncertainty associated with <img src="https://render.githubusercontent.com/render/math?math=\epsilon" />, the irreducible error.</p><p>The standard error can also be used to perform hypothesis testing on the estimated coefficients. The most common hypothesis test called the <strong>t-test</strong>  involves testing the null hypothesis <em>H0</em> against the hypothesis <em>H1</em>, stated as:</p><ul><li><em>H0: There is no relationship between X and Y</em>,</li><li><em>H1: There is some relationship between X and Y.</em></li></ul><p>In mathematical terms, the null hypothesis corresponds to testing if <img src="https://render.githubusercontent.com/render/math?math=\beta_1 = 0" />, which reduces to</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=y = \beta_0 %2B \epsilon" />,
</p><p>in other words this tests that X is not related to Y.
To test the null hypothesis, it is necessary to determine whether the estimate of <img src="https://render.githubusercontent.com/render/math?math=\beta_1" />, <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta_1}" />, is sufficiently far from zero to provide confidence that <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta_1}" /> is non-zero.</p><p>What is close enough depends on <img src="https://render.githubusercontent.com/render/math?math=SE(\hat{\beta_1})" />. When <img src="https://render.githubusercontent.com/render/math?math=SE(\hat{\beta_1})" /> is small, then small values of <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta_1}" /> may provide strong evidence that <img src="https://render.githubusercontent.com/render/math?math=\beta_1" /> is not zero. Conversely, if <img src="https://render.githubusercontent.com/render/math?math=SE(\hat{\beta_1})" />. When <img src="https://render.githubusercontent.com/render/math?math=SE(\hat{\beta_1})" /> is large, then <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta_1}" /> will need to be large in order to reject the null hypothesis.</p><p>In practice, computing a <strong>t-statistic</strong>, which measures the number of standard deviations that <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta_1}" />, is away from 0, is useful for determining if an estimate is sufficiently significant to reject the null hypothesis. The <em>t-Statistic</em> is computed as follows:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=t = \frac{\hat{\beta_1} - 0}{SE(\hat{\beta_1})}" /></p><p>If there is no relationship between X and Y, it is expected that a t-distribution with n−2 degrees of freedom should be yielded.</p><p>With such a distribution, it is possible to calculate the probability of observing a value of <img src="https://render.githubusercontent.com/render/math?math=|t|" /> or larger assuming that <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta_1} = 0" />. This probability, called the <strong>p-value</strong>, can indicate an association between the predictor and the response if sufficiently small.</p><p><strong>How to assess model accuracy?</strong> Having rejected the null hypothesis, it is often useful to quantify the goodness of fit of the model (the extent to which the model fits the data). For a linear regression model typically the <strong>residual standard error</strong> along with the <strong>r-squared</strong> statistic.</p><p>The residual standard error is an estimate of the standard deviation of <img src="https://render.githubusercontent.com/render/math?math=\epsilon" />, the irreducible error. In other words the RSE is the average amount by which the response will deviate from the true regression line. The residual standard error can be computed as:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=RSE = \sqrt{\frac{1}{n-2}RSS} =\sqrt{\frac{1}{n-2}\sum_{i=1}^{n}(y_i - \hat{y_i})^2}" /></p><p>The residual standard error is a measure of the lack of fit of the model to the data. A large value indicates the model does not fit the data well. A difficulty using the residual standard error however is that the magnitude is in units of Y so determining a good RSE value is not intuitive.</p><p>The  statistic is an alternative measure of fit that takes the form of a proportion. The R2 statistic captures the proportion of variance explained as a value between 0 and 1, independent of the unit of Y.</p><p>The <img src="https://render.githubusercontent.com/render/math?math=R^2" /> statistic provides a measure of fit as a proportion of variance explained as a value between 0 and 1. It is calculated as follows:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=R^2 = \frac{TSS - RSS}{TSS} = 1 - \frac{RSS}{TSS}" /></p><p>where <img src="https://render.githubusercontent.com/render/math?math=RSS = \sum_{i=1}^{n}(y_i - \hat{y_i})^2" />, and <img src="https://render.githubusercontent.com/render/math?math=TSS = \sum_{i=1}^{n}(y_i - \bar{y_i})^2" />.</p><p>The total sum of squares, TSS, measures the total variance in the response Y. The TSS can be thought of as the total variability in the response before applying linear regression. Conversely, the residual sum of squares, RSS, measures the amount of variability left after performing the regression. Ergo, TSS−RSS measures the amount of variability in the response that is explained by the model. An r-squared value close to 1 indicates that a large proportion of the bariability in the response is explained by the model, conversely a value close to 0 indicates that the model accounted for very little of the variability.</p><p>An r-squared value close to 0 may be because the linear model is a bad assumption or because the standard deviation (squared) is high. Although r-squared is easy to interpret as it is confined between 0 and 1, determining what constitutes an acceptable r-squared is non-trivial and depends on the specifics of the problem.</p><p><strong>Correlation</strong> is another measure of the linear relationship between X and Y, calculated as:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=Cor(X, Y) = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i-1}{n}(x_i - \bar{x})^2}\sqrt{\sum_{i-1}{n}(y_i - \bar{y})^2}}" /></p><p>This suggests that <img src="https://render.githubusercontent.com/render/math?math=r=Cor(X, Y)" /> could be used instead of r-squared to assess the goodness of fit for a linear model. Correlation does not extend to multiple linear regression since correlation quantifies the association between a single pair of variables, however the r-squared statistic can.</p><p><strong>Multiple linear regression</strong> models extend simple linear regression to accommodate multiple predictors in the form:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=Y= \beta_0 %2B \beta_1X_1 %2B \beta_2X_2 %2B ... %2B \beta_pX_p" /></p><p>As the coefficients <img src="https://render.githubusercontent.com/render/math?math=\beta_0, \beta_1, \beta_2, %2B ... %2B \beta_p" /> are not known, it is necessary to estimate their values <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta_0}, \hat{\beta_1}, \hat{\beta_2}, %2B ... %2B \hat{\beta_p}" /> giving:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=Y= \hat{\beta_0} %2B \hat{\beta_1}X_1 %2B \hat{\beta_2}X_2 %2B ... %2B \hat{\beta_p}X_p" /></p><p>The coefficients can be estimated using the same least squares approach as in simple linear regression, to minimise the sum of squares, using matrix algebra:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=RSS = \sum_{i=1}^{n}(y_i - \hat{y_i})^2 = \sum_{i=1}^{n}(y_i - \hat{\beta_0} %2B \hat{\beta_1}X_1 %2B \hat{\beta_2}X_2 %2B ... %2B \hat{\beta_p}X_p)^2" /></p><p><strong>Coefficient accuracy is assessed in multiple regressions</strong> by testing the null hypothesis <em>H0</em> against the hypothesis <em>H1</em>, stated as:</p><ul><li><em>H0</em>: <img src="https://render.githubusercontent.com/render/math?math=H_0 : \beta_1 = \beta_2 = ... = \beta_p = 0" /></li><li><em>H1</em>: <img src="https://render.githubusercontent.com/render/math?math=H_a: at least one of B_j \neq 0 " /></li></ul><p>Here the <strong>F-statistic</strong> can be used to determine which hypothesis is true, and is computed as:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=F = \frac{(TSS - RSS)/p}{RSS/(n - p - 1)} = \frac{\frac{TSS - RSS}{p}}{\frac{RSS}{n - p - 1}}" /></p><p>where again,  <img src="https://render.githubusercontent.com/render/math?math=RSS = \sum_{i=1}^{n}(y_i - \hat{y_i})^2" />, and <img src="https://render.githubusercontent.com/render/math?math=TSS = \sum_{i=1}^{n}(y_i - \bar{y_i})^2" />..</p><p>If the assumptions of the linear model, represented by the alternative hypothesis, are true it can be shown that:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=E\{\frac{RSS}{n - p - 1}\} = \sigma^{2}" /></p><p>and conversely for the null-hypothesis if true, it can be shown that:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=E\{\frac{TSS - RSS}{p}\} = \sigma^{2}" /></p><p>When there is no relationship between the response and the predictors the <em>F-statisitic</em> has a value close to 1, and conversely, if the alternative hypothesis is true, then the <em>F-statistic</em> will take on a value greater than 1. When n is large, an F-statistic only slightly greater than 1 may provide evidence against the null hypothesis. If n is small, a large <em>F-statistic</em> is needed to reject the null hypothesis.</p><p>When the null hypothesis is true and the errors <img src="https://render.githubusercontent.com/render/math?math=\epsilon_i" /> have a normal distribution, the <em>F-statistic</em> follows an <em>F-distribution</em>. Using the <em>F-distribution</em>, it is possible to figure out a <em>p-value</em> for the given <em>n</em>, <em>p</em>, and <em>F-statistic</em>. Based on the obtained <em>p-value</em>, the validity of the null hypothesis can be determined.</p><p>It is sometimes desirable to test that a particular subset of <em>q</em> coefficients are 0. This equates to a null hypothesis of</p><ul><li><em>H0</em>: <img src="https://render.githubusercontent.com/render/math?math=H_0 : \beta_{p-q+1} = \beta_{p-q+2} = ... = \beta_{p} = 0" /></li></ul><p>Supposing that the residual sum of squares for such a model is <img src="https://render.githubusercontent.com/render/math?math=RSS_0" /> then the F-statistic could be calculated as:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=F = \frac{(RSS_0 - RSS)/q}{RSS/(n - p - 1)} = \frac{\frac{RSS_0 - RSS}{q}}{\frac{RSS}{n - p - 1}}" /></p><p>Even in the presence of p-values for each individual variable, it is still important to consider the overall F-statistic because there is a reasonably high likelihood that a variable with a small p-value will occur just by chance, even in the absence of any true association between the predictors and the response.</p><p>In contrast, the F-statistic does not suffer from this problem because it adjusts for the number of predictors. The F-statistic is not infallible and when the null hypothesis is true the F-statistic can still result in p-values below 0.05 about 5% of the time regardless of the number of predictors or the number of observations.</p><p>The F-statistic works best when p is relatively small or when p is relatively small compared to n.
When p is greater than n, multiple linear regression using least squares will not work, and similarly, the F-statistic cannot be used either.</p><p><strong>Selecting important variables</strong> is the next step after establishing that at least one of the predicotrs is associated with the response. <em>Variable selection</em> is the process of removing extraneous predictors that don't relate to the response.</p><p>An exhaustive search would test different subsets of predictors and selecting the best model as determined by various statistical methods. However, there are <img src="https://render.githubusercontent.com/render/math?math=2^p" /> subset of predictors so this is often infeasible. An efficient and automated means of choosing a smaller subset of models is needed, there are a number of approaches to limiting the range of possible models:</p><ul><li><em>Forward selection</em> is a greedy approach starting with a model having an intercept but no predictors, and attempts to add predictors and perform linear regressions, keeping whichever predictor results in the lowest residual sum of squares. The predictor yielding the lowest RSS is added to the model one-by-one until some halting condition is met.</li><li><em>Backward selection</em> begins with a model having all the predictors and proceeds to remove variables with the highest p-value each iteration until some stopping condition is met. Backwards selection cannot be used when p&gt;n.</li><li><em>Mixed selection</em> starts with a null model and, repeatedly adds whichever predictor yields the best fit. As more predictors are added, the p-values become larger. When this happens, if the p-value for one of the variables exceeds a certain threshold, that variable is removed from the model. The selection process continues in this forward and backward manner until all the variables in the model have sufficiently low p-values and all the predictors excluded from the model would result in a high p-value if added to the model.</li></ul><p>In addition to r-squared and RSE, it is also useful to plot the data to verify the model. However, it is important to note that these predictions will be subject to three types of uncertainty:</p><ol><li>The coefficient estimates, <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta_0}, \hat{\beta_1}, ..., , \hat{\beta_p}" />, are only estimates of the actual coefficients <img src="https://render.githubusercontent.com/render/math?math=\beta_0, \beta_1, ..., , \beta_p" />. The error introduced by this inaccuracy is reducible error and a confidence interval can be computed to determine how close <img src="https://render.githubusercontent.com/render/math?math=\hat{y}" /> is to f(X).</li><li>The assumption of a linear model for f(X) is almost always an approximation of reality, which means additional reducible error is introduced due to model bias. A linear model often models the best linear approximation of the true, non-linear surface.</li><li>Even in the case where f(X) and the true values of the coefficients are known, the response value cannot be predicted exactly because of the random, irreducible error <img src="https://render.githubusercontent.com/render/math?math=\epsilon" />.</li></ol><p><strong>Qualitative predictors</strong> can also be accommodated in  linear regression by introducing an indicator (dummy) variable, that takes a numerical encoding, as follows:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=X_i = \big\{ \begin{array}{cc}{1\quad if\quad p_i = class \A}\\{0\quad if\quad p_i = class \B}\end{array}" /></p>
this yields a regression equation as:
<p align="center"><img src="https://render.githubusercontent.com/render/math?math=y_i = \beta_0 %2B \beta_1 X_1 %2B \epsilon_i = \big\{ \begin{array}{cc}{\beta_0 %2B \beta_1 %2B \epsilon_i  \quad if\quad p_i = class \A}\\{\beta_0 %2B \epsilon_i \quad if\quad p_i = class \B}\end{array}" /></p><p>Given such a coding, <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta_1}" /> represents the average difference in  <img src="https://render.githubusercontent.com/render/math?math=X_1" /> between classes A and B.</p><p>When a qualitative predictor takes on more than two values, a single dummy variable cannot represent all possible values. Instead, multiple dummy variables can be used. The number of variables required will always be one less than the number of values that the predictor can take on.</p><p>For example, with a predictor that can take on three values, the following coding could be used</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=X_{i1} = \big\{ \begin{array}{cc}{1\quad if\quad p_i = class \A}\\{0\quad if\quad p_i \ne class \A}\end{array}" /></p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=X_{i2} = \big\{ \begin{array}{cc}{1\quad if\quad p_i = class \B}\\{0\quad if\quad p_i \ne class \B}\end{array}" /></p>
this yields a regression equation as:
<p align="center"><img src="https://render.githubusercontent.com/render/math?math=y_i = \beta_0 %2B \beta_1 X_1  %2B \beta_2 X_2 %2B \epsilon_i = \big\{ \begin{array}{cc}{\beta_0 %2B \beta_1 %2B \epsilon_i  \quad if\quad p_i = class \A}\\{\beta_0 %2B \beta_2  %2B \epsilon_i \quad if\quad p_i = class \B}\\{\beta_0 %2B \epsilon_i \quad if\quad p_i = class \C}\end{array}" /></p><p>With such a coding, <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta_0}" /> can be interpreted as the average response for class C. <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta_1}" /> can be interpreted as the average difference in response between classes A and C. Finally, <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta_2}" /> can be interpreted as the average difference in response between classes B and C.</p><p>The case where <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta_1}" /> and <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta_2}" /> are both zero, the level with no dummy variable, is known as the baseline.</p><p>Using dummy variables allows for easily mixing quantitative and qualitative predictors. There are many ways to encode dummy variables. Each approach yields equivalent model fits, but results in different coefficients and different interpretations that highlight different contrasts.</p><p><strong>Predictor interactions and extending the linear model</strong>, restrictive assumptions are made in linear regressions. Principally the <em>additive assumption</em> which assumes that the relationship between the predictors and response is additive, another assumption is that the relationship between the predictors and the response is linear.</p><p>An additive assumption implies that the effect of changes in a predictor on the response is independent of the other predictors, which is often not the case. A linear assumption implies that the change in the response due to a change in predictor is constant. <strong>Interaction terms</strong> can be added to overcome the limitations of the additive model, for example:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=Y = \beta_0  %2B \beta_1 X_1 %2B \beta_2 X_2 %2B \beta_3 X_1 X_2 %2B \epsilon" /></p><p>The hierarchical principle states that, when an interaction term is included in the model, the main effects should also be included, even if the p-values associated with their coefficients are not significant. The reason for this is that  <img src="https://render.githubusercontent.com/render/math?math=X_1 X_2" /> is often correlated with <img src="https://render.githubusercontent.com/render/math?math=X_1" /> and  <img src="https://render.githubusercontent.com/render/math?math=X_2" /> and removing them tends to change the meaning of the interaction.</p><p>If  <img src="https://render.githubusercontent.com/render/math?math=X_1 X_2" /> is related to the response, then whether or not the coefficient estimates of <img src="https://render.githubusercontent.com/render/math?math=X_1" /> or <img src="https://render.githubusercontent.com/render/math?math=X_2" /> are exactly zero is of limited interest.</p><p>Interaction terms can also model a relationship between a quantitative predictor and a qualitative predictor.</p><p><strong>Non-Linear relationships</strong> can be modelled to mitigate the effects of the linear assumption by incorporating polynomial functions of the predictors in the regression model. For example, in a scenario where a quadratic relationship seems likely, the following model could be used</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=Y = \beta_0  %2B \beta_1 X_1 %2B \beta_2 X_1^2 %2B \epsilon" /></p><p>This extension of the linear model to accommodate non-linear relationships is called <strong>polynomial regression</strong>.</p><p><strong>Problems with linear regression:</strong></p><ol><li><p><strong>Non-linearity</strong>: If the true relationship between the response and predictors is far from linear, then virtually all conclusions that can be drawn from the model are suspect and prediction accuracy can be significantly reduced. <strong>Residual plots</strong> are useful for identifying non-linearity.</p></li><li><p><strong>Correlation of error terms</strong>: An important assumption of linear regression is that the error terms, are uncorrelated. If the error terms are correlated it will result in incorrect standard error values that will tend to underestimate the true standard error. This will result in prediction intervals and confidence intervals that are narrower than they should be. In addition, p-values associated with the model will be lower than they should be. In other words, correlated error terms can make a model appear to be stronger than it really is. Correlations in error terms can be the result of time series data, unexpected observation relationships, and other environmental factors. Observations that are obtained at adjacent time points will often have positively correlated errors. Good experimental design is a crucial factor in limiting correlated error terms.</p></li><li><p><strong>Non-constant variance of error terms</strong>: linear regression also assumes that the error terms have a constant variance,</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=Var(\epsilon_i_) = \sigma^2" /></p>
 Standard errors, confidence intervals, and hypothesis testing all depend on this assumption.
</li><li><p><strong>Outliers</strong> an outlier is an example for which yi is far from the value predicted by the model. Excluding outliers can result in improved residual standard error and improved r-squared values, usually with negligible impact to the least squares fit.</p><p>Residual plots can help identify outliers, though it can be difficult to know how big a residual needs to be before considering a point an outlier. To address this, it can be useful to plot the studentized residuals instead of the normal residuals. Studentized residuals are computed by dividing each residual, <img src="https://render.githubusercontent.com/render/math?math=\epsilon_i" />, by its estimated standard error. Observations whose studentized residual is greater than |3| are possible outliers.</p><p>Outliers should only be removed when confident that the outliers are due to a recording or data collection error since outliers may otherwise indicate a missing predictor or other deficiency in the model.</p></li><li><p><strong>High-leverage points</strong> While outliers relate to observations for which the response yi is unusual given the predictor xi, in contrast, observations with high leverage are those that have an unusual value for the predictor xi for the given response yi.</p><p>High leverage observations tend to have a sizable impact on the estimated regression line and as a result, removing them can yield improvements in model fit.</p><p>For simple linear regression, high leverage observations can be identified as those for which the predictor value is outside the normal range. With multiple regression, it is possible to have an observation for which each individual predictor’s values are well within the expected range, but that is unusual in terms of the combination of the full set of predictors.</p><p>To qualify an observation’s leverage, the leverage statistic can be computed.</p></li><li><p><strong>Collinearity</strong> refers to the situation in which two or more predictor variables are closely related to one another. Collinearity can pose problems for linear regression because it can make it hard to determine the individual impact of collinear predictors on the response.</p><p>Collinearity reduces the accuracy of the regression coefficient estimates, which in turn causes the standard error of <img src="https://render.githubusercontent.com/render/math?math=\beta_j" /> to grow. Since the T-statistic for each predictor is calculated by dividing <img src="https://render.githubusercontent.com/render/math?math=\beta_j" /> by its standard error, collinearity results in a decline in the true T-statistic. This may cause it to appear that <img src="https://render.githubusercontent.com/render/math?math=\beta_j" /> and <img src="https://render.githubusercontent.com/render/math?math=x_j" /> are related to the response when they are not. As such, collinearity reduces the effectiveness of the null hypothesis. Because of all this, it is important to address possible collinearity problems when fitting the model.</p><p>One way to detect collinearity is to generate a correlation matrix of the predictors. Any element in the matrix with a large absolute value indicates highly correlated predictors. This is not always sufficient though, as it is possible for collinearity to exist between three or more variables even if no pair of variables have high correlation. This scenario is known as multicollinearity.</p><p>Multicollinearity can be detected by computing the variance inflation factor. The variance inflation factor is the ratio of the variance of <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta_j}" /> when fitting the full model divided by the variance of <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta_j}" /> if fit on its own. The smallest possible VIF value is 1.0, which indicates no collinearity whatsoever. In practice, there is typically a small amount of collinearity among predictors. As a general rule of thumb, VIF values that exceed 5 or 10 indicate a problematic amount of collinearity.</p></li></ol><p><strong>Parametric methods versus non-parametric methods</strong>. A non-parametric method akin to linear regression is k-nearest neighbors regression which is closely related to the k-nearest neighbors classifier. A parametric approach will outperform a non-parametric approach if the parametric form is close to the true form of f(X). The choice of a parametric approach versus a non-parametric approach will depend largely on the bias-variance trade-off and the shape of the function f(X).
When the true relationship is linear, it is difficult for a non-parametric approach to compete with linear regression because the non-parametric approach incurs a cost in variance that is not offset by a reduction in bias. Additionally, in higher dimensions, K-nearest neighbors regression often performs worse than linear regression. This is often due to combining too small an n with too large a p, resulting in a given observation having no nearby neighbors. This is often called the <strong>curse of dimensionality</strong>.</p><p>As a general rule, parametric models will tend to outperform non-parametric models when there are only a small number of observations per predictor.</p><p><b>Further reading:</b></p><ul><li><a href="https://medium.com/@erika.dauria/looking-at-r-squared-721252709098" target="_blank">Looking at r-squared.</a></li><li><a href="https://math.stackexchange.com/questions/63238/why-do-we-use-a-least-squares-fit" target="_blank">Why do we use a least squuares fit?</a>.</li><li><a href="https://youtu.be/ewnc1cXJmGA" target="_blank">Deriving the least squares estimators of the slope and intercept.</a></li><li><a href="https://youtu.be/rODUBTRUV0U" target="_blank">Deriving the mean and variance of the least squares slope estimator in simple linear regression</a></li></ul><h2 id="classification">Classification:</h2><p>Linear regression is an apt classifier in the scenario of binary qualitative responses, however beyond two levels difficulties arise. The choice of coding scheme is problematic, with different coding schemes yielding different predictions.</p><p><strong>Logistic regression</strong> models the probability that <img src="https://render.githubusercontent.com/render/math?math=y" /> belongs to a particular category (not the response itself). The <em>logistic function</em> ensures the prediction is between 0 and 1, and is given as:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=p(X) = \frac{e^{\beta_0 %2B \beta_1 X}}{1 %2B  e^{\beta_0 %2B \beta_1 X}}" /></p><p>after re-balancing:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\frac{p(X)}{1 - p(X)} =e^{\beta_0 %2B \beta_1 X}" /></p><p>The left side of the above equation is known as the odds and takes a value between zero and infinity. The <em>logit</em> function or <em>log odds</em> takes the logarithm of both sides:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=log \big[\frac{p(X)}{1 - p(X)} \big]=\beta_0 %2B \beta_1 X" /></p><p>Logistic regression has a logit function that is linear in terms of X. Unlike linear regression where <img src="https://render.githubusercontent.com/render/math?math=\beta_1" /> represents the average change in Y with a one-unit increase in X, for logistic regression, increasing X by one-unit yields a <img src="https://render.githubusercontent.com/render/math?math=\beta_1" /> change in the log-odds which is equivalent to multiplying the odds by <img src="https://render.githubusercontent.com/render/math?math=\epsilon^{\beta_1}" />.</p><p>The relationship between <em>p(X)</em> and <em>X</em> is not linear, so <img src="https://render.githubusercontent.com/render/math?math=\beta_1" /> does not correspond to the change in <em>p(X)</em> given one-unit increase in <em>X</em>. However, if <img src="https://render.githubusercontent.com/render/math?math=\beta_1" /> is positive, increasing <em>X</em> will be associated with an increase in <em>p(X)</em> and, similarly, if <img src="https://render.githubusercontent.com/render/math?math=\beta_1" /> is negative, an increase in <em>X</em> will be associated with a decrease in <em>p(X)</em>. How much change will depend on the value of <em>X</em>.</p><p><strong>Estimating regression coefficients</strong> for logistic regression uses a strategy called <strong>maximum likelihood</strong>.</p><p>The maximum likelihood determines estimates for <img src="https://render.githubusercontent.com/render/math?math=\beta_0" /> and <img src="https://render.githubusercontent.com/render/math?math=\beta_1" /> such that the predicted probability of <img src="https://render.githubusercontent.com/render/math?math=\hat{p}(x_i)" /> corresponds with the observed classes as closely as possible. Formally, this yield an equation called a likelihood function:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\ell(\beta_0, \beta_1)) \displaystyle \prod_{i:y_{i}=1}p(X_{i})\times \displaystyle \prod_{j:y_{j}=0}(1-p(X_{j}))" /></p><p>Estimates for <img src="https://render.githubusercontent.com/render/math?math=\beta_0" /> and<img src="https://render.githubusercontent.com/render/math?math=\beta_1" /> are chosen so as to maximize this likelihood function. Linear regression’s least squares approach is actually a special case of maximum likelihood.</p><p>Logistic regression measures the accuracy of coefficient estimates using a quantity called the <strong>z-statistic</strong>, which is similar to the t-statistic. The z-statistic for <img src="https://render.githubusercontent.com/render/math?math=\beta_1" /> is represented by</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\normalsize \textrm{z-statistic}(\beta_{1}) = \frac{\hat{\beta}_{1}}{\mathrm{SE}(\hat{\beta}_{1})}" /></p><p>A large z-statistic supports the hypothesis, where in logistic regression, the null hypothesis is:</p><ul><li><em>H0</em>: <img src="https://render.githubusercontent.com/render/math?math=\beta_1 = 0" />
implies that</li></ul><img src="https://render.githubusercontent.com/render/math?math=p(X)=\frac{\epsilon^{\beta_0}}{1 %2B \epsilon^{\beta_0}}" /><p>and, therefore, p(X) does not depend on X.</p><p><em>Predictions with logistic regression</em> are made plugging in the estimates found from the maximum likelihood estimation in to the model equation</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\hat{p}(X) = \frac{e^{\hat{\beta_0} %2B \hat{\beta_1} X}}{1 %2B  e^{\hat{\beta_0} %2B \hat{\beta_1} X}}" /></p><p>Generally the estimated intercept, <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta_0}" /> captures the ratio of positive and negative classifications in the given data set, and is not directly interpretable in most situations.</p><p><strong>Multiple logistic regression</strong> is an extension of logistic regression with more predictors, that uses a strategy similar to that employed for linear regression, multiple logistic regression can be generalised as,</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=log \big[\frac{p(X)}{1 - p(X)} \big]=\beta_0 %2B \beta_1 X_1 %2B ... %2B \beta_p X_p" /></p><p>The log-odds equation for multiple logistic regression can be expressed as</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=p(X)=\frac{\epsilon^{\beta_0 %2B \beta_1 X_1 %2B ... %2B \beta_p X_p}}{1 %2B \epsilon^{\beta_0 %2B \beta_1 X_1 %2B ... %2B \beta_p X_p}}" /></p><p>Maximum likelihood is also used to estimate <img src="https://render.githubusercontent.com/render/math?math=\beta_0, \beta_1, ..., \beta_p" /> in the case of multiple logistic regression.</p><p>In general, the scenario in which the result obtained with a single predictor does not match the result with multiple predictors, especially when there is correlation among the predictors, is referred to as <strong>confounding</strong>. More specifically, confounding describes situations in which the experimental controls do not adequately allow for ruling out alternative explanations for the observed relationship between the predictors and the response.</p><p>Though multiple-class logistic regression is possible, discriminant analysis tends to be the preferred means of handling multiple-class classification.</p><p><strong>Linear discriminant analysis (LDA)</strong> is a technique to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.</p><p>Logistic regression models the conditional distribution of the response Y given the predictors X, whilst linear discriminant analysis takes the approach of modeling the distribution of the predictors X separately in each of the response classes , Y, and then uses Bayes’ theorem to invert these probabilities to estimate the conditional distribution.</p><p>Linear discriminant analysis is popular when there are more than two response classes. It has some advantages over logistic regression:</p><ul><li>The parameter estimates for logistic regression can be surprisingly unstable when the response classes are well separated. Linear discriminant analysis does not suffer from this problem.</li><li>Logistic regression is more unstable than linear discriminant analysis when n is small and the distribution of the predictors X is approximately normal in each of the response classes.</li></ul><p>Classification With Bayes’ Theorem
Assuming a qualitative variable Y that can take on <img src="https://render.githubusercontent.com/render/math?math=K \geq 2" /> distinct, unordered values, the prior probability describes the probability that a given observation is associated with the kth class of the response variable Y.</p><p>The density function of X for an observation that comes from the kth class is defined as</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=f_k(X) = Pr(X=x|Y=k)" /></p><p>This means that <img src="https://render.githubusercontent.com/render/math?math=f_k(X)" /> should be relatively large if there’s a high probability that an observation from the kth class features X=x. Conversely, <img src="https://render.githubusercontent.com/render/math?math=f_k(X)" /> will be relatively small if it is unlikely that an observation in class k would feature X=x.</p><p>Following this intuition, Bayes’ theorem states</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=Pr(Y=k|X=x)=p_k(X)=\displaystyle \frac{\pi_k f_k(X)}{\sum_{j=1}^k\pi_j f_j(X)}" /></p><p>where <img src="https://render.githubusercontent.com/render/math?math=\pi_k" /> denotes the prior probability that the chosen observation comes from the kth class. This equation is sometimes abbreviated as <img src="https://render.githubusercontent.com/render/math?math=p_k(x)" />.</p><p><img src="https://render.githubusercontent.com/render/math?math=p_k(X)=Pr(Y=k|X)" /> is also known as the posterior probability, or the probability that an observation belongs to the kth class, given the predictor value for that observation.</p><p>Estimating <img src="https://render.githubusercontent.com/render/math?math=\pi_k" />, the prior probability, is easy given a random sample of responses from the population.</p><p>Estimating the density function, <img src="https://render.githubusercontent.com/render/math?math=f_k(X)" /> tends to be harder, but making some assumptions about the form of the densities can simplify things. A good estimate for <img src="https://render.githubusercontent.com/render/math?math=f_k(X)" /> allows for developing a classifier that approximates the Bayes’ classifier which has the lowest possible error rate since it always selects the class for which <img src="https://render.githubusercontent.com/render/math?math=p_k(x)" />  is largest.</p><p><strong>Linear discriminant analysis for one predictor</strong> we assume that <img src="https://render.githubusercontent.com/render/math?math=f_k(X)" /> has a normal distribution, or Gaussian distribution, the normal density is described by</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=f_k(X) = \frac{1}{2\sqrt{2\pi}\sigma_k} exp[-\frac{1}{2\sigma_k^2}(x - \mu_k)^2]" /></p><p>where <img src="https://render.githubusercontent.com/render/math?math=\mu_k" /> is the mean parameter for the kth class and <img src="https://render.githubusercontent.com/render/math?math=\sigma_k^2" /> is the variable parameter for the kth class.</p><p>The density function can be further simplified by assuming that the variance terms,</p><p><img src="https://render.githubusercontent.com/render/math?math=\sigma_1^2, ..., \sigma_k^2" />, are all equal in which case the variance is denoted by <img src="https://render.githubusercontent.com/render/math?math=\sigma_1^2, ..., \sigma^2" />.
Plugging the simplified normal density function into Bayes’ theorem yields</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=p_k(x)= \displaystyle  \frac{\pi_k \frac{1}{\sqrt{2\pi}\sigma} exp[-\frac{1}{2\sigma_k^2}(x - \mu_k)^2]}{\sum_{j=1}^k\pi_j \frac{1}{\sqrt{2\pi}\sigma} exp[-\frac{1}{2\sigma_k^2}(x - \mu_j)^2]}" /></p><p>It can be shown that by taking a log of both sides and removing terms that are not class specific, a simpler equation can be extracted:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\delta_{k}(x) = \frac{x\mu_{k}}{\sigma^{2}} - \frac{\mu_{k}^{2}}{2\sigma^{2}} + \log(\pi_{k})" /> 
</p><p>Using this equation, an observation can be classified by taking the class yields the largest value.</p><p>Linear discriminant analysis uses the following estimated values for <img src="https://render.githubusercontent.com/render/math?math=\mu^k" /> and <img src="https://render.githubusercontent.com/render/math?math=\sigma^2" />:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\hat{\mu}_{k} = \frac{1}{n_{k}}\sum_{i:y_{i} = k}x_{i}" /></p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\sigma^{2} = \frac{1}{n - k} \displaystyle \sum_{k=1}^{k} \displaystyle \sum_{i:y_{i} = k} (x_{i} - \mu_{k})^{2}" /></p><p>where n is the total number of training observations and <img src="https://render.githubusercontent.com/render/math?math=n_k" /> is the number of training observations in class k.</p><p>The estimate of <img src="https://render.githubusercontent.com/render/math?math=\mu^k" /> is the average value of x for all training observations in class k.</p><p>The estimate of <img src="https://render.githubusercontent.com/render/math?math=\sigma^2" /> can be seen as a weighted average of the sample variance for all k classes.</p><p>When the class prior probabilities, <img src="https://render.githubusercontent.com/render/math?math=\pi_1, ... \pi_n" />, is not known, it can be estimated using the proportion of training observations that fall into the kth class:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\hat{\pi_k} = \frac{n_k}{n}" /></p><p>Plugging the estimates for
<img src="https://render.githubusercontent.com/render/math?math=\hat{\mu}_k" /> and <img src="https://render.githubusercontent.com/render/math?math=\hat{\sigma}_k^2" /> into the modified Bayes’ theorem yields the linear discriminant analysis classifer:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\hat{\delta}_{k}(x) = \frac{x\hat\mu_{k}}{\hat\sigma^{2}} - \frac{\hat\mu_{k}^{2}}{2\hat\sigma^{2}} + \log(\hat\pi_{k})" /> 
</p><p>which assigns an observation X=x to whichever class yields the largest value.</p><p>This classifier is described as linear because the discriminant function <img src="https://render.githubusercontent.com/render/math?math=\hat\sigma_k(x)" /> is linear in terms of x and not a more complex function.</p><p>The Bayes decision boundary for linear discriminant analysis is identified by the boundary where <img src="https://render.githubusercontent.com/render/math?math=\delta_k(x) = \delta_j(x)" />.
The linear discriminant analysis classifier assumes that the observations from each class follow a normal distribution with a class specific average vector and constant variance, <img src="https://render.githubusercontent.com/render/math?math=\sigma^2" />, and uses these simplifications to build a Bayes’ theorem based classifier.</p><p><strong>Multivariate linear discriminant analysis</strong> assumes that <img src="https://render.githubusercontent.com/render/math?math=X=(X_1, X_2 , ..., X_p)" /> comes from a multivariate normal distribution with a class-specific mean vector and a common covariance matrix.</p><p>The multivariate Gaussian distribution used by linear discriminant analysis assumes that each predictor follows a one-dimensional normal distribution with some correlation between the predictors. The more correlation between predictors, the more the bell shape of the normal distribution will be distorted.</p><p>A p-dimensional variable X can be indicated to have a multivariate Gaussian distribution with the notation <img src="https://render.githubusercontent.com/render/math?math=X ~ N(\mu, \Sigma)" />, where <img src="https://render.githubusercontent.com/render/math?math=E(x)=\mu" /> is the mean of X (a vector with p components) and <img src="https://render.githubusercontent.com/render/math?math=Cov(X)=\Sigma" /> is the p x p covariance matrix of X.</p><p>Multivariate Gaussian density is formally defined as:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\normalsize f(x) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}} \exp \big \lgroup -\frac{1}{2}(x - \mu)^{T}\Sigma^{-1}(x - \mu) \big \rgroup" /></p><p>For linear discriminant analysis with multiple predictors, the multivariate Gaussian distribution, <img src="https://render.githubusercontent.com/render/math?math=N(\mu_k, \Sigma)" />, is assumed to have a class specific mean vector, <img src="https://render.githubusercontent.com/render/math?math=\mu_k" />, and a covariance vector common to all classes, <img src="https://render.githubusercontent.com/render/math?math=\Sigma" />.</p><p>Combining the multivariate Gaussian density function with Bayes’ theorem yields the vector/matrix version of the linear discriminant analysis Bayes’ classifier:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\normalsize \delta_{k}(x) = x^{T} \Sigma^{-1} \mu_{k} - \frac{1}{2} \mu_{k}^{T} \Sigma^{-1} \mu_{k} %2B \log \pi_{k}" /></p>
Again, whichever class yields the largest value is the highest probability classification.
<p>The Bayes decision boundaries are defined by the values for which</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\normalsize x^{T} \Sigma^{-1} \mu_{j} - \frac{1}{2} \mu_{j}^{T} \Sigma^{-1}\mu_{j} = x^{T} \Sigma^{-1} \mu_{k} - \frac{1}{2} \mu_{k}^{T} \Sigma^{-1} \mu_{k}" /></p><p>It should be noted that since all classes are assumed to have the same number of training observations, the
<img src="https://render.githubusercontent.com/render/math?math=log \pi" /> terms cancel out.</p><p>As was the case for one-dimensional linear discriminant analysis, it is necessary to estimate the unknown parameters <img src="https://render.githubusercontent.com/render/math?math=\mu_1,..., \mu_k" /> and <img src="https://render.githubusercontent.com/render/math?math=\pi_1,..., \pi_k" />
and <img src="https://render.githubusercontent.com/render/math?math=\Sigma" />. The formulas used in the multi-dimensional case are similar to those used with just a single dimension.</p><p>Since, even in the multivariate case, the linear discriminant analysis decision rule relates to X in a linear fashion, the name linear discriminant analysis holds.</p><p>As with other methods, the higher the ratio of parameters, p, to number of samples, n, the more likely overfitting will occur.</p><p>In general, binary classifiers are subject to two kinds of error: false positives and false negatives. A confusion matrix can be a useful way to display these error rates. Class-specific performance is also important to consider because in some cases a particular class will contain the bulk of the error.</p><p>In medicine and biology, the term sensitivity refers to the percentage of observations correctly positively classified (true positives) and specificity refers to the percentage of observations correctly negatively classified (true negatives).</p><p>In a two-class scenario, the Bayes classifier, and by extension, linear discriminant analysis, uses a 50% threshold for the posterior probability when determining classifications. In some cases it may be desirable to lower this threshold.</p><p>A ROC curve is a useful graphic for displaying the two types of error rates for all possible thresholds. ROC is a historic acronym that comes from communications theory and stands for receiver operating characteristics.</p><p>The overall performance of a classifier summarized over all possible thresholds is quantified by the area under the ROC curve.</p><p><strong>Quadratic discriminant analysis</strong> offers an alternative approach to linear discriminant analysis that makes most of the same assumptions, except that quadratic discriminant analysis assumes that each class has its own covariance matrix. This amounts to assuming that an observation from the kth class has a distribution of the form <img src="https://render.githubusercontent.com/render/math?math=X~N(\mu_k, \Sigma_k)" /> where <img src="https://render.githubusercontent.com/render/math?math=\Sigma_k" /> is a covariance matrix for class k.</p><p>This yields a Bayes classifier that assigns an observation X=x to the class with the largest value for</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\normalsize \delta_{k}(x) = - \frac{1}{2}(x - \mu_{k})^{T} \Sigma_{k}^{-1} (x - \mu_{k}) - \frac{1}{2} \log |\Sigma_{k}| %2B log \pi_{k}" /></p><p>which is equivalent to</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math= \normalsize \delta_{k}(x) = - \frac{1}{2}x^{T} \Sigma_{k}^{-1} %2B x^{T} \Sigma_{k}^{-1}\mu_{k} - \frac{1}{2}\mu_{k}^{T} \Sigma_{k}^{-1} \mu_{k} - \frac{1}{2} \log | \Sigma_{k} | %2B \log \pi_{k}" /></p><p>The quadratic discriminant analysis Bayes classifier gets its name from the fact that it is a quadratic function in terms of x.
The choice between a shared covariance matrix (like that assumed in linear discriminant analysis) and a class-specific covariance matrix (like that assumed in quadratic discriminant analysis) amounts to a bias-variance trade-off. This is because when there are p predictors, estimating a covariance matrix requires estimating <img src="https://render.githubusercontent.com/render/math?math=\frac{p(p+1)}{2}" /> parameters. Since quadratic discriminant analysis estimates a separate covariance matrix for each class, this amounts to estimating <img src="https://render.githubusercontent.com/render/math?math=\frac{kp(p+1)}{2}" /> parameters.</p><p>By assuming a common covariance matrix, linear discriminant analysis is linear in terms of x which means Kp linear coefficients must be estimated. Because of this, linear discriminant analysis is much less flexible than quadratic discriminant analysis, but as a result has much lower variance. If the assumption of a common covariance matrix is highly inaccurate, it can cause linear discriminant analysis to suffer from high bias.</p><p>In general terms, linear discriminant analysis tends to be a better choice if the importance of reducing variance is important because there are relatively few training examples. Conversely, quadratic discriminant analysis can be a better choice if the training set is large such that the variance of the classifier is not a concern or if the assumption of a common covariance matrix is not realistic.</p><p><strong>Comparing the models</strong>
Since logistic regression and linear discriminant analysis are both linear in terms of x, the primary difference between the two methods is their fitting procedures. Linear discriminant analysis assumes that observations come from a Gaussian distribution with a common covariance matrix, and as such, out performs logistic regression in cases where these assumptions hold true.</p><p>K-nearest neighbors can outperform linear regression and linear discriminant analysis when the decision boundary is highly non-linear, but at the cost of a less interpretable model.</p><p>Quadratic discriminant analysis falls somewhere between the linear approaches of linear discriminant analysis and logistic regression and the non-parametric approach of K-nearest neighbors. Since quadratic linear analysis models a quadratic decision boundary, it has more capacity for modeling a wider range of problems.</p><p>Quadratic discriminant analysis is not as flexible as K-nearest neighbors, however it can perform better than K-nearest neighbors when there are fewer training observations due to its high bias.</p><p>Linear discriminant analysis and logistic regression will perform well when the true decision boundary is linear.</p><p>Quadratic discriminant analysis may give better results when the decision boundary is moderately non-linear.</p><p>Non-parametric approaches like K-nearest neighbors may give better results when the decision boundary is more complex and the right level of smoothing is employed.</p><p>As was the case in the regression setting, it is possible to apply non-linear transformations to the predictors to better accommodate non-linear relationships between the response and the predictors.</p><p>The effectiveness of this approach will depend on whether or not the increase in variance introduced by the increase in flexibility is offset by the reduction in bias.</p><p>It is possible to add quadratic terms and cross products to the linear discriminant analysis model such that it has the same form as quadratic discriminant analysis, however the parameter estimates for each of the models would be different. In this fashion, it’s possible to build a model that falls somewhere between linear discriminant analysis and quadratic discriminant analysis.</p><p><b>Further reading:</b></p><ul><li><a href="https://youtu.be/XepXtl9YKwc" target="_blank">Maximum likelihoood clearly explained!</a></li></ul><h2 id="resampling-methods">Resampling methods:</h2><p>In statistics, resampling is a process of repeatedly drawing samples from a population, to estimate the precision of sample statistics, or validating models by using random subsets. Model assessment is the process of evaluating a model’s performance. Model selection is the process of selecting the appropriate level of flexibility for a model.</p><p>Resampling methods can be expensive since they require repeatedly performing the same statistical methods on N different subsets of the data.</p><p><strong>Cross validation</strong> is a resampling method that can be used to estimate a given statistical methods test error or to determine the appropriate amount of flexibility.</p><p>The validation set approach involves randomly dividing the available observations into two groups, a training set and a validation or hold-out set. The model is then fit using the training set and then the fitted model is used to predict responses for the observations in the validation set. The resulting validation set error rate offers an estimate of the test error rate.</p><p>There are two potential drawbacks to this approach. First, the estimated test error rate can be highly variable depending on which observations fall into the training set and which observations fall into the test/validation set. Second, the estimated error rate tends to be overestimated since the given statistical method was trained with fewer observations than it would have if fewer observations had been set aside for validation.</p><p>Cross-validation is a refinement of the validation set approach that mitigates these two issues.</p><p><strong>Leave-one-out cross validation</strong>
Leave-one-out cross validation is similar to the validation set approach, except instead of splitting the observations evenly, leave-one-out cross-validation withholds only a single observation for the validation set. This process can be repeated n times with each observation being withheld once. This yields n mean squared errors which can be averaged together to yield the leave-one-out cross-validation estimate of the test mean squared error.</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=CV(n) = \frac{1}{n} \sum_{i=1}^{n} MSE_i" /></p><p>Leave-one-out cross validation has much less bias than the validation set approach. Leave-one-out cross validation also tends not to overestimate the test mean squared error since many more observations are used for training. In addition, leave-one-out cross validation is much less variable, in fact, it always yields the same result since there’s no randomness in the set splits.</p><p>Leave-one-out cross validation can be expensive to implement since the model has to be fit n times. This can be especially expensive in situations where n is very large and/or when each individual model is slow to fit.</p><p>A shortcut exists for least squares linear or polynomial regression that makes the cost of leave-one-out cross validation the same as a single model fit. Formally stated, the shortcut is</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=CV(n) = \frac{1}{n} \sum_{i=1}^{n} [\frac{y_i-\hat{y_i}}{1 - n_i}]^2" /></p><p>where <img src="https://render.githubusercontent.com/render/math?math=\hat{y_i}" /> is the ith fitted value from the least squares fit and <img src="https://render.githubusercontent.com/render/math?math=n_i" /> is the leverage statistic, defined as</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=n_i = \frac{1}{n} %2B frac{(x_i - \bar{x})^2}{\sum_{j=1}{n}(x_j - \bar{x})^2}" /></p><p>for a simple linear regression.</p><p>Because the ith residual is divided by <img src="https://render.githubusercontent.com/render/math?math=1-n_i" />, each observation is inflated based on the amount the observation influences its own fit which allows the inequality to hold.</p><p>Leave-one-out cross validation is a very good general method which can be used with logistic regression, linear discriminant analysis, and many other methods. That said, the shortcutting method doesn’t hold in general which means the model generally needs to be refitted n times.</p><p><strong>K-fold cross validation</strong> operates by randomly dividing the set of observations into K groups or folds of roughly equal size, each of the K folds is used as the validation set while the other K−1 folds are used as the test set to generate K estimates of the test error. The K-fold cross validation estimated test error comes from the average of these estimates.</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=CV(k) = \frac{1}{k} \sum_{i=1}^{k} [\frac{y_i-\hat{y_i}}{1 - n_i}]^2" /></p><p>It can be shown that leave-one-out cross validation is a special case of K-fold cross validation where <img src="https://render.githubusercontent.com/render/math?math=K=n" />. Typical values for K are 5 or 10 since these values require less computation than when K is equal to n.
Cross validation can be used both to estimate how well a given statistical learning procedure might perform on new data and to estimate the minimum point in the estimated test mean squared error curve, which can be useful when comparing statistical learning methods or when comparing different levels of flexibility for a single statistical learning method.</p><p>There is a <strong>bias-variance trade-off inherent to the choice of K in K-fold cross validation</strong>. Typically, values of K=5 or K=10 are used as these values have been empirically shown to produce test error rate estimates that suffer from neither excessively high bias nor very high variance.</p><p>In terms of bias, leave-one-out cross validation is preferable to K-fold cross validation and K-fold cross validation is preferable to the validation set approach.</p><p>In terms of variance, K-fold cross validation where <img src="https://render.githubusercontent.com/render/math?math=K \lt n" /> is preferable to leave-one-out cross validation and leave-one-out cross validation is preferable to the validation set approach.</p><p>The <strong>bootstrap</strong> is a widely applicable tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning approach, including those for which it is difficult to obtain a measure of variability.</p><p>The bootstrap generates distinct data sets by repeatedly sampling observations from the original data set. These generated data sets can be used to estimate variability in lieu of sampling independent data sets from the full population.</p><p>The sampling employed by the bootstrap involves randomly selecting n observations with replacement, which means some observations can be selected multiple times while other observations are not included at all.</p><p>This process is repeated B times to yield B bootstrap data sets, <img src="https://render.githubusercontent.com/render/math?math=Z^{*1}, Z^{*2}, ..., Z^{*B}" />, which can be used to estimate other quantities such as standard error.</p><p>For example, the estimated standard error of an estimated quantity <img src="https://render.githubusercontent.com/render/math?math=\hat{\alpha}" /> can be computed using the bootstrap as follows:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=SE_B(\hat{\alpha}) \sqrt{\frac{1}{B-1} \sum_{r=1}^{B} (\hat{\alpha^{*r}}- \frac{1}{B}\sum_{s=1}^{B})}\hat{\alpha^{*s}})^2" /></p><p><b>Further reading:</b></p><ul><li><a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)" target="_blank">Bootstrapping wikipedia</a></li></ul><h2 id="linear-model-selection-and-regularization">Linear Model Selection and Regularization:</h2><p>Fitting procedures other than least squares can yield better model interpretability and predictive accuracy.</p><ul><li>Prediction accuracy: the bias of least squares will be low if the true relationship between the response and the predictors is approximately linear. If <img src="https://render.githubusercontent.com/render/math?math=n \gt\gt p" />, least squares estimates will tend to have low variance too. Otherwise there can be too much variability to the least squares fit leading to over-fitting and poor predictions. When <img src="https://render.githubusercontent.com/render/math?math=n \lt p" /> no unique least squares estimate exists, and therefore least squares cannot be used. Constraining or shrinking the estimated coefficients can reduce variance while incurring only a negligible increase in bias.</li><li>Interpretability: not all variables in a multiple regression will be associated with the response. Irrelevant variables add complexity and reduce interpretability. Feature selection can automatically exclude irrelevant variables froma  multiple regression model.</li></ul><p><strong>Subset selection</strong> is the process of identifying the subset of variables that are related to the response.</p><ul><li><p><strong>Best subset selection</strong> fits all <img src="https://render.githubusercontent.com/render/math?math=2^p" /> possible combinations of predictors and then selects the best model. Determining the best model is not trivial and involves the following:</p><ol><li>Let <img src="https://render.githubusercontent.com/render/math?math=M_0" /> denote the null model which uses no predictors and always yields the sample mean for predictions</li><li>For <img src="https://render.githubusercontent.com/render/math?math=K = 1, 2, ..., p" />:
<ol><li>Fit all <img src="https://render.githubusercontent.com/render/math?math=\binom{p}{k}" /> models that contain exactly k predictors.</li><li>Let <img src="https://render.githubusercontent.com/render/math?math=M_k" /> denote the <img src="https://render.githubusercontent.com/render/math?math=\binom{p}{k}" /> model that yields the smallest residual sum of squares or equivalently the largest <img src="https://render.githubusercontent.com/render/math?math=R^2" />.</li><li>Select the best model from <img src="https://render.githubusercontent.com/render/math?math=M_0, ..., M_p" /> using cross-validated prediction error, Akaike information criterion, Bayes information criterion, or adjusted <img src="https://render.githubusercontent.com/render/math?math=R^2" />.</li></ol></li></ol><p>Step 3 of the above should be performed carefully as the number of features used by the models increases, the residual sum of squares decreases monotonically and the <img src="https://render.githubusercontent.com/render/math?math=R^2" /> increases monotonically. Hence, statistically the best model will always involve all of the variables, however this does not always transfer to a low test error.</p><p>Best subset selection has computational limitations since <img src="https://render.githubusercontent.com/render/math?math=2^p" /> models must be considered. Stepwise methods are an alternative to overcoming these computational limitations.</p></li><li><p><strong>Forward stepwise selection</strong> begins with a model that uses no predictors and successively adds predictors in a greedy manner, selecting that which yields the greatest improvement, until all are used.</p><ol><li>Let <img src="https://render.githubusercontent.com/render/math?math=M_0" /> denote the null model which uses no predictors</li><li>For <img src="https://render.githubusercontent.com/render/math?math=K = 0, 1,..., (p-1)" />:
<ol><li>Consider all (p-k) modesl that augment the predictors of <img src="https://render.githubusercontent.com/render/math?math=M_k" /> with one additional parameter.</li><li>Choose the best (p-k) model that yields the smallest residual sum of squares or largest r-squared and call it <img src="https://render.githubusercontent.com/render/math?math=M_{k+1}" /></li></ol></li><li>Select a single best model from the models <img src="https://render.githubusercontent.com/render/math?math=M_0, M_1, ... M_p" /> using cross-validated prediction error, Akaike information criterion, Bayes information cirterion or adjsuted r-squared.</li></ol><p>Forward stepwise selection fits <img src="https://render.githubusercontent.com/render/math?math=1 %2B \frac{p(p+1)}{2}" /> models which is a significant improvement over best subset selection <img src="https://render.githubusercontent.com/render/math?math=2^p" /> models. Forward stepwise selection may not always yield the best possible model due to its additive (greedy) nature. It can be used in high-dimensional scernarios where <img src="https://render.githubusercontent.com/render/math?math=n \lt p" />.</p></li><li><p><strong>Backward stepwise selection</strong> is another efficient subset selection variant, starting with the full least squares model,  and iteratively removing the least useful predictor with each iteration.</p><ol><li>Let <img src="https://render.githubusercontent.com/render/math?math=M_p" /> denote a model using all predictors.</li><li>For <img src="https://render.githubusercontent.com/render/math?math=k = p, p-1,...,1" /><ol><li>Consider all k models that use k=1 predictors from <img src="https://render.githubusercontent.com/render/math?math=M_k" />.</li><li>Choose the best of these k models as determined by the smallest residual sum of squares or highest r-squared, call this model <img src="https://render.githubusercontent.com/render/math?math=M_{k-1}" /></li></ol></li><li>Select the single best model from <img src="https://render.githubusercontent.com/render/math?math=M_0, ..., M_p" /> using cross-validated prediction error, AIC, BIC, or adjusted r-squared</li></ol><p>Similar to forward stepwise selection, backward stepwise selection is not guaranteed to yield the best result. It does not work in high-dimensional scenarios where <img src="https://render.githubusercontent.com/render/math?math=n \lt p" /> so the full model can be fit. Both forward stepwise selection and backward stepwise selection perform a guided search over the model space.</p></li><li><p><strong>Hybrid approaches</strong> methods add variables to the model sequentially, analogous to forward stepwise selection, but with each iteration they may also remove any variables that no longer offer any improvement to model fit. Hybrid approaches try to better simulate best subset selection while maintaining the computational advantages of stepwise approaches.</p></li></ul><p><strong>Choosing an optimal model</strong> since <img src="https://render.githubusercontent.com/render/math?math=R^2" /> and RSS are both related to training error, the model with all the predictors will always appear to be the best. To combat this, it would be better to select the model from the set of models that yields the lowest estimated test error. Two common approaches for estimating test error are:</p><ol><li>Indirectly estimating test error by making adjustments to the training error to account for the bias caused by overfitting.</li><li>Directly estimating test error using a validation set or cross validation.</li></ol><p><strong>Cp, AIC, BIC, and adjusted r-squared</strong>
Training mean squared error (RSS / n) usually underestimates test mean squared error since the least squares approach ensures the smallest training residual sum of squares. An important difference being that training error will decrease as variables are added, where test error may not decrease. THis prevents the use of training residual sum of squares and the <img src="https://render.githubusercontent.com/render/math?math=R^2" /> for comparing models with different numbers of variables.</p><p>There are, however, a number of techniques for adjusting training error according to model size which enables comparing models with different numbers of variables. Four of these strategies are: Cp, Akaike information criterion, Bayes information criterion, and adjusted R2.</p><p>The Cp estimate of test mean squared error for a model containing d predictors fitted with least squares is calculated as:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=C_p = \frac{1}{n}(RSS %2B 2d\hat{\sigma}^2)" /></p><p>where <img src="https://render.githubusercontent.com/render/math?math=\hat{\sigma}^2" /> is an estimate of the variance of the error <img src="https://render.githubusercontent.com/render/math?math=\epsilon" /> associated with each response measurement. So the Cp statistics adds a penalty of <img src="https://render.githubusercontent.com/render/math?math=2d\hat{\sigma}^2" /> to the training residual sum of squares to adjust for the tendency for training error to underestimate test error and adjust for additional predictors. It can be shown that if<br /><img src="https://render.githubusercontent.com/render/math?math=\hat{\sigma}^2" /> is an unbiased estimate of <img src="https://render.githubusercontent.com/render/math?math=\sigma^2" />, then Cp will be an unbiased estimate of test mean squared error. As a result, Cp tends to take on small values when test mean square error is low, so a model with a low Cp is preferable.</p><p><strong>Akaike information criterion (AIC)</strong> is defined for a large class of models fit by maximum likelihood. In the case of simple linear regression, when errors follow a Gaussian distribution, maximum likelihood and least squares are the same thing, in which case, AIC is given by:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=AIC = \frac{1}{n\hat{\sigma}^{2}}(RSS %2B 2d\hat{\sigma}^{2})" /></p><p>Cp and AIC are proportional for least squares models and as such AIC offers no benefit in this case.</p><p><em>Bayes information criterion (BIC)</em> for least squares models with d predictors is given by:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=BIC = \frac{1}{n}(RSS %2B log(n) d \hat{\sigma}^{2})" /></p><p>Similar to Cp, BIC tends to take on smaller values when test MSE is low, so smaller values of BIC are preferable. The BIC statistic tends to penalize models with more variables more heavily than Cp, which in turn results in the selection of smaller models.</p><p><em>Adjusted r-squared</em> is another popular choice for comparing models with differing numbers of variables. For a least squares fitted model with d predictors, adjusted r-squared is given by:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=Adjusted\ R^{2} = 1 - \frac{RSS/(n - d - 1)}{TSS/(n - 1)}" /></p><p>For adjusted r-squared a larger value signifies a lower test error. Adjusted r-squared aims to penalize models that include unnecessary variables. This stems from the idea that after all of the correct variables have been added, adding additional noise variables will only decrease the residual sum of squares slightly. Validation and cross validation can be useful in situations where it’s hard to pinpoint the model’s degrees of freedom or when it’s hard to estimate the error variance. The one-standard-error rule advises that when many models have low estimated test error and it’s difficult or variable as to which model has the lowest test error, one should select the model with the fewest variables that is within one standard error of the lowest estimated test error. The rationale being that given a set of more or less equally good models, it’s often better to pick the simpler model.</p><p><strong>Shrinkage</strong> is an alternative to subset selection that uses all the predictors, but employs a technique to constrain or regularize the coefficient estimates. Constraining coefficient estimates can significantly reduce their variance. Two well known techniques of shrinking regression coefficients toward zero are ridge regression and the lasso.</p><p><strong>Ridge regression</strong> is a method of shrinkage very similar to least squares fitting except the coefficients are estimated by minimizing a modified quantity. Recall that the least squares fitting procedure estimates the coefficients by minimizing the residual sum of squares where the residual sum of squares is given by</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=RSS = \sum_{i=1}^{n} \bigg\lgroup y_{i} - \beta_{0} - \sum_{j=1}^{p}\beta_{j}X_{ij} \bigg\rgroup ^{2}" /></p><p>Ridge regression instead selects coefficients by selecting coefficients that minimize</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=RSS %2B \lambda\sum_{j=1}^{p}\beta_{j}^{2}" /></p><p>where <img src="https://render.githubusercontent.com/render/math?math=\lambda" /> is a tuning parameter, and the second sum term is a shrinkage penalty. The tuning parameter serves to control the balance of how the two terms affect coefficient estimates. When λ is zero, the second term is nullified, yielding estimates exactly matching those of least squares. As λ approaches infinity, the impact of the shrinkage penalty grows, pushing/shrinking the ridge regression coefficients closer and closer to zero.</p><p>The <img src="https://render.githubusercontent.com/render/math?math=\ell {2}" /> norm of a vector is defined as</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\|\beta\|_{2} = \sqrt{\sum_{j=1}^{p}\beta_{j}^{2}}" /></p>
which measures the distance of vector <img src="https://render.githubusercontent.com/render/math?math=\beta" /> from zero. In regard to ridge regression, as λ increases, the <img src="https://render.githubusercontent.com/render/math?math=\ell_{2}" /> norm of <im src="https://render.githubusercontent.com/rpender/math?math=\beta^R_{\lambda}"> will always decrease as the coefficient estimates shrink closer to zero.
<p>An important difference between ridge regression and least squares regression is that least squares regression’s coefficient estimates are scale equivalent and ridge regression’s are not. This means that multiplying X by a constant, C, leads to a scaling of the least squares coefficient estimates by a factor of <img src="https://render.githubusercontent.com/render/math?math=\frac{1}{C}" />. Another way of looking at it is that regardless of how the jth predictor is scaled, the value of <img src="https://render.githubusercontent.com/render/math?math=X_j \beta_j" /> remains the same. In contrast, ridge regression coefficients can change dramatically when the scale of a given predictor is changed. This means that <img src="https://render.githubusercontent.com/render/math?math=X_j \hat{\beta}_{\lambda}^{R}" /> may depend on the scaling of other predictors. Because of this, it is best to apply ridge regression after standardizing the predictors using the formula below:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\widetilde{x}_{ij} =\frac{x_{ij}}{\sqrt{\frac{1}{n}\sum_{i=1}^{n}(x_{ij} - \bar{x}_{j})^{2}}}" /></p><p>This formula puts all the predictors on the same scale by normalizing each predictor relative to its estimated standard deviation. As a result, all the predictors will have a standard deviation of 1 which will yield a final fit that does not depend on the scale of the predictors.</p><p>Ridge regression’s advantage over least squares stems from the bias-variance trade-off. As the tuning parameter λ increases, the flexibility of the ridge regression fit decreases leading to a decrease in variance, but also causing an increase in bias. Since least squares is equivalent to the most flexible form of ridge regression (where λ=0) it offers less bias at the cost of higher variance. As such, ridge regression is best employed in situations where least squares estimates have high variance.</p><p>Ridge regression also offers computational advantages for fixed values of λ. In fact, it can be shown that the computational requirements of calculating ridge regression coefficient estimates for all values of λ simultaneously are almost identical to those for fitting a model using least squares.</p><p>Compared to subset methods, ridge regression is at a disadvantage when it comes to number of predictors used since ridge regression will always use all p predictors. Ridge regression will shrink predictor coefficients toward zero, but it will never set any of them to exactly zero (except when λ=∞ ). Though the extra predictors may not hurt prediction accuracy, they can make interpretability more difficult, especially when p is large.</p><p><strong>The lasso</strong> a more recent alternative to ridge regression that allows for excluding some variables. Coefficient estimates for the lasso are generated by minimizing the quantity</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=RSS %2B \lambda\sum_{j=1}^{p}\|\beta_{j}\|" /></p><p>The main difference between ridge regression and lasso regression is that lasso regression uses the <img src="https://render.githubusercontent.com/render/math?math=\ell_{1}" /> norm of the coefficient vector <img src="https://render.githubusercontent.com/render/math?math=\beta" /> as its penalty. So instead of having a squared term there is an absolute term. Practically this means that in lasso regression if <img src="https://render.githubusercontent.com/render/math?math=\lambda" /> is sufficiently large then coefficient estimates can be forced to zero. These models are sometimes called sparse models as since they include only a subset of variables. The variable selection of the lasso can be considered a kind of soft thresholding.</p><p>Selecting the tuning parameter, λ, can be accomplished for both ridge regression and the lasso through the use of cross-validation. A general algorithm for selecting a tuning parameter might proceed like so:</p><ol><li>Select a range of λ values</li><li>Compute the cross-validation error for the given shrinkage method for each value of λ.</li><li>Select the value of λ for which the cross-validation error is the smallest.</li><li>Refit the model using all available observations and the selected tuning parameter value.</li></ol><p>Neither ridge regression nor the lasso is universally dominant over the other. The lasso will perform better in scenarios where not all of the predictors are related to the response, or where some number of variables are only weakly associated with the response. Ridge regression will perform better when the response is a function of many predictors, all with coefficients roughly equal in size.</p><p>Like ridge regression, the lasso can help reduce variance at the expense of a small increase in bias in situations where the least squares estimates have excessively high variance.</p><p><strong>Dimension reduction methods</strong> are techniques that transform the predictors then fit a least squares model using the transformed variables instead of the original predictors. Let <img src="https://render.githubusercontent.com/render/math?math=Z_1, Z_2, ..., Z_m" /> represent <img src="https://render.githubusercontent.com/render/math?math=M \lt P" /> linear combinations of the original predictors, p. Formally,</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=Z_{m} = \sum_{j=1}^{p} \phi_{jm}X_{j}" /></p>
 For some constants <img src="https://render.githubusercontent.com/render/math?math=\phi_{1m}, \phi_{2m}, ..., \phi_{pm}" />, <img src="https://render.githubusercontent.com/render/math?math=m =1, ..., M" />. It is then possible to use least squares to fit the linear regression model:
<p align="center"><img src="https://render.githubusercontent.com/render/math?math=y_{i} = \theta_{0} + \sum_{m=1}^{M} \theta_{m}Z_{im} + \epsilon_{i}" /></p><p>where <img src="https://render.githubusercontent.com/render/math?math=i=1,...,n" /> and the regression coefficients are represented by <img src="https://render.githubusercontent.com/render/math?math=\theta_0, \theta_1, ..., , \theta_M" />. If the constants <img src="https://render.githubusercontent.com/render/math?math=\phi_{1M}, \phi_{2M}, ..., , \phi_{pm}" /> are chosen carefully, dimension reduction approaches can outperform least squares regression of the original predictors. The term dimension reduction references the fact that this approach reduces the problem of estimating the p+1 coefficients <img src="https://render.githubusercontent.com/render/math?math=\theta_0, \theta_1, ..., , \theta_M" />, where M &lt; p, there by reducing the dimension of the problem from P+1 to M+1.</p><p>This approach can be seen as a special constrained version of the original linear regression considering that</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\sum_{m=1}^{M} \theta_{m}Z_{im} = \sum_{m=1}^{M} \theta_{m} \sum_{j=1}^{p} = \phi_{jm} X_{ij} = \sum_{j=1}^{p} \sum_{m=1}^{M}\theta_{m}\phi_{jm}X_{ij} = \sum_{j=1}^{p}\beta_{j}X_{ij}" /></p><p>where <img src="https://render.githubusercontent.com/render/math?math=\beta_{j} = \sum_{m=1}^{M}\theta_{m}\phi_{jm}" />, this serves to constrain the estimated <img src="https://render.githubusercontent.com/render/math?math=\beta_j" /> coefficients since they must now take the form <img src="https://render.githubusercontent.com/render/math?math=\beta_{j} = \sum_{m=1}^{M}\theta_{m}\phi_{jm}" />.</p><p>This constraint has the potential to bias the coefficient estimates, but in situations where p is large relative to n, selecting a value of M much less than p can significantly reduce the variance of the fitted coefficients.</p><p>If M=p and all the linear combinations <img src="https://render.githubusercontent.com/render/math?math=Z_M" />  are linearly independent, the constraint has no effect and no dimension reduction occurs and the model is equivalent to performing least squares regression on the original predictors.</p><p>All dimension reduction methods work in two steps. First, the transformed predictors, <img src="https://render.githubusercontent.com/render/math?math=Z_1, Z_2, ..., Z_M" /> are obtained. Second, the model is fit using the M transformed predictors.</p><p>The difference in dimension reduction methods tends to arise from the means of deriving the transformed predictors, <img src="https://render.githubusercontent.com/render/math?math=Z_1, Z_2, ..., Z_M" /> or the selection of the <img src="https://render.githubusercontent.com/render/math?math=\phi_{jm}" /> coefficients.</p><p>Two popular forms of dimension reduction are principal component analysis and partial least squares.</p><p><strong>Principal component regression</strong> uses principal component analysis to derive a low dimensional set of features from a large set of variables</p><p>Principal component analysis (PCA) is a technique for reducing the dimension of an n×p data matrix X.
The first principal component direction of the data is the line along which the observations vary the most.</p><p>Put another way, the first principal component direction is the line such that if the observations were projected onto the line then the projected observations would have the largest possible variance and projecting observations onto any other line would yield projected observations with lower variance.</p><p>Another interpretation of principal component analysis describes the first principal component vector as the line that is as close as possible to the data. In other words, the first principal component line minimizes the sum of the squared perpendicular distances between each point and the line. This means that the first principal component is chosen such that the projected observations are as close as possible to the original observations.</p><p>Projecting a point onto a line simply involves finding the location on the line which is closest to the point.</p><p>For a two predictor scenario, the first principal component can be summarized mathematically as</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=Z_{1} = \phi_{11} \times (x_{1j} - \bar{x}_{1}) %2B \phi_{21} \times (x_{2j} - \bar{x}_{2})" /></p><p>where <img src="https://render.githubusercontent.com/render/math?math=\phi^2_11 %2B \phi^2_21 = 1" />
and for which the selected values of <img src="https://render.githubusercontent.com/render/math?math=\phi_{11}" /> and <img src="https://render.githubusercontent.com/render/math?math=\phi_{21}" /> maximize the variance of the linear combination.</p><p>It is necessary to consider only linear combinations of the form <img src="https://render.githubusercontent.com/render/math?math=\phi^2_11 %2B \phi^2_21 = 1" /> because otherwise <img src="https://render.githubusercontent.com/render/math?math=\phi_{11}" /> and <img src="https://render.githubusercontent.com/render/math?math=\phi_{21}" /> could be increased arbitrarily to exaggerate the variance.</p><p>In general, up to min(p,n−1) distinct principal components can be constructed.</p><p>The second principal component, <img src="https://render.githubusercontent.com/render/math?math=Z_2" /> is a linear combination of the variables that is uncorrelated with <img src="https://render.githubusercontent.com/render/math?math=Z_1" /> and that has the largest variance subject to that constraint.</p><p>It turns out that the constraint that <img src="https://render.githubusercontent.com/render/math?math=Z_2" /> must not be correlated with <img src="https://render.githubusercontent.com/render/math?math=Z_1" /> is equivalent to the condition that the direction of <img src="https://render.githubusercontent.com/render/math?math=Z_2" /> must be perpendicular, or orthogonal, to the first principal component direction.</p><p>Generally, this means</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=Z_{2} = \phi_{21} \times (x_{2} - \bar{x}_{2}) - \phi_{11} \times (x_{1} - \bar{x}_{1})" /></p><p>Constructing additional principal components, in cases where p&gt;2, would successively maximize variance subject to the constraint that the additional components be uncorrelated with the previous components.</p><p><strong>The principal component regression approach</strong> first constructs the first M principal components, <img src="https://render.githubusercontent.com/render/math?math=Z_1, Z_2, ..., Z_M" />, and then uses the components as the predictors in a linear regression model that is fit with least squares.</p><p>The premise behind this approach is that a small number of principal components can often suffice to explain most of the variability in the data as well as the relationship between the predictors and the response. This relies on the assumption that the directions in which <img src="https://render.githubusercontent.com/render/math?math=X_1,..., X_p" /> show the most variation are the directions that are associated with the predictor Y. Though not always true, it is true often enough to approximate good results.</p><p>In scenarios where the assumption underlying principal component regression holds true, then the result of fitting a model to <img src="https://render.githubusercontent.com/render/math?math=Z_1, Z_2, ..., Z_M" /> will likely be better than the result of fitting a model to <img src="https://render.githubusercontent.com/render/math?math=X_1,..., X_p" /> since most of the information in the data that relates to the response is captured by <img src="https://render.githubusercontent.com/render/math?math=Z_1, Z_2, ..., Z_M" /> and by estimating only M≪p coefficients overfitting is mitigated.</p><p>The number of principal components used relates to the bias-variance trade-off in that using fewer principal components will increase bias, but reduce variance and conversely, using more principal components will decrease bias, but increase variance.</p><p>Principal component regression will tend to do better in scenarios where fewer principal components are sufficient to capture most of the variation in the predictors and the relation with response. The closer M is to p, the more principal component regression will resemble the results of a least squares model fit to the original predictors.</p><p>It should be noted that principal component regression is not a feature selection method since each of the M principal components used in the regression is a linear combination of all p original predictors. For this reason, principal component regression is more similar to ridge regression than it is to the lasso. In fact, it can be shown that principal component regression and ridge regression are closely related with ridge regression acting as a continuous version of principle component regression.</p><p>As with the shrinkage methods, the value chosen for M is best informed by cross-validation.</p><p>It is generally recommended that all predictors be standardized prior to generating principal components. As with ridge regression, standardization can be achieved via</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\widetilde{x}_{ij} = \frac{X_{ij}}{\sqrt{\frac{1}{n}\sum_{i=1}^{n}(X_{ij} - \bar{x}_{j})^{2}}}" /></p><p>Standardization ensures all variables are on the same scale which limits the degree to which the high-variance predictors dominate the principal components obtained. Additionally, the scale on which the variables are measured will ultimately affect the principal component regression model obtained. That said, if the variables are all in the same units, one might choose not to standardize them.</p><p>Partial Least Squares
Unlike principal component regression, partial least squares is a supervised learning method in that the value of the response is used to supervise the dimension reduction process.</p><p><strong>Partial least squares (PLS)</strong> identifies a new set of features <img src="https://render.githubusercontent.com/render/math?math=Z_1, Z_2, ..., Z_M" /> that are linear combinations of the original predictors and then uses these M new features to fit a linear model using least squares.</p><p>Unlike principal component regression, partial least squares makes use of the response Y to identify new features that not only approximate the original predictors well, but that are also related to the response.</p><p>The first partial least squares component is computed by first standardizing the p predictors. Next, the values of each <img src="https://render.githubusercontent.com/render/math?math=\phi_{j1}" /> coefficient is set by performing a simple linear regression of Y onto <img src="https://render.githubusercontent.com/render/math?math=X_j" />. It can be shown that the derived coefficient is proportional to the correlation between Y and <img src="https://render.githubusercontent.com/render/math?math=X_j" />. Because of this proportional relationship, it can be seen that partial least squares places the highest weight on variables that are most strongly related to the response as it computes</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=Z_{1} = \sum_{j=1}^{p} \phi_{j1}X_{j}" /></p><p>To identify the second partial least squares direction, it is first necessary to adjust each of the variables for Z1. This is achieved by regressing each variable onto Z1 and taking the residuals. These residuals can be interpreted as the remaining information not explained by the first partial least squares direction.</p><p>This orthogonalized data is used to compute Z2 in the same way that the original data was used to compute Z1. This iterative approach can be repeated M times to identify multiple partial least squares components, <img src="https://render.githubusercontent.com/render/math?math=Z_1, Z_2, ..., Z_M" />.
Like principal component regression, the number of partial least squares directions, M, used with partial least squares is generally selected using cross validation.</p><p>Before performing partial least squares, the predictors and the response are typically standardized.</p><p>In practice, partial least squares often performs no better than principal component regression or ridge regression. Though the supervised dimension reduction of partial least squares can reduce bias, it also has the potential to increase variance. Because of this, the benefit of partial least squares compared to principal component regression is often negligible.</p><p><strong>Considerations for high-dimensional data</strong>, since most  statistical techniques for regression and classification are intended for low dimensional settings where p≪n. Data containing more features than observations are often referred to as high-dimensional. When p≥n, least squares will yield a set of coefficient estimates that perfectly fit the data whether or not there is truly a relationship between the features and the response. As such, least squares should never be used in a high-dimensional setting.</p><p>Cp, AIC, and BIC are also not appropriate in the high-dimensional setting because estimating <img src="https://render.githubusercontent.com/render/math?math=\sigma^2" /> is problematic.</p><p><strong>Regression in high dimensions</strong>. Methods for generating less flexible least squares models like forward stepwise selection, ridge regression, and the lasso turn out to be especially useful in the high-dimensional setting, since they essentially avoid overfitting by using a less flexible fitting approach.</p><p>Regularization and/or shrinkage play a key role in high-dimensional problems.</p><p>Appropriate tuning parameter selection is crucial for good predictive performance in the high-dimensional setting.</p><p>Test error tends to increase as the dimension of the problem increases unless the additional features are truly associated with the response. This is related to the curse of dimensionality, as the additional noise features increase the dimensionality of the problem, increasing the risk of overfitting without any potential upside.</p><p>The risk of multicollinearity is also exacerbated in the high-dimensional setting. Since any variable in the model can be written as a linear combination of all the other variables in the model, it can be extremely difficult to determine which variables (if any) are truly predictive of the outcome. Even the best regression coefficient estimates can never be identified. The best that can be hoped for is that large regression coefficients are assigned to the variables that are truly predictive of the outcome.</p><p>In the high-dimensional setting, one should never use sum of squared errors, p-values, r-squared, or other traditional measures of model fit on the training data as evidence of good model fit. MSE or r-squared of an independent test set is a valid measure of model fit, but MSE or r-squared of the training set is certainly not.</p><h2 id="moving-beyond-linearity">Moving Beyond Linearity:</h2><p>These methods extend linear models by relaxing the linearity assumption whilst attempting to maintain model interpretability:</p><ul><li><p>Polynomial regression extends the linear model by adding additional predictors obtained by raising each of the original predictors to a power. For example, cubic regression uses three variables, <img src="https://render.githubusercontent.com/render/math?math=X, X^2, and X^3" /> as predictors.</p></li><li><p>Step functions split the range of a variable into k distinct regions in order to produce a qualitative variable. This has the effect of fitting a piecewise constant function.</p></li><li><p>Regression splines are an extension of polynomials and step functions that provide more flexibility. Regression splines split the range of X into k distinct regions and within each region a polynomial function is used to fit the data. The polynomial functions selected are constrained to ensure they join smoothly at region boundaries called knots. With enough regions, regression splines can offer an extremely flexible fit.</p></li><li><p>Smoothing splines are similar to regression splines, but unlike regression splines, smoothing splines result from minimizing a residual sum of squares criterion subject to a smoothness penalty.</p></li><li><p>Local regression is similar to splines, however the regions are allowed to overlap in the local regression scenario. The overlapping regions allow for improved smoothness.</p></li><li><p>Generalized additive models extend splines, local regression, and polynomials to deal with multiple predictors.</p></li></ul><p><strong>Polynoial regresion</strong> extends linear regression to accommodate scenarios where the relationship between the predictors and the response is non-linear, with the form:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=y_{i} = \beta_{0} %2B \beta_{1}X_{i} %2B \beta_{2}X_{i}^{2} + \beta_{3}X_{i}^{3} %2B ... %2B \beta_{d}X_{i}^{d} %2B \epsilon_{i}" /></p><p>The degree <em>d</em> is the highest power to which the the polynomial is raised. This form can produce extremely non-linear curves, so a degree greater than 3 or 4 is rarely used. Above this and the model is overly flexible and can take on some strange shapes especially near the boundaries of the variable X.</p><p>As in linear regression the coefficients in polynomial regression can be estimated easily using least squares linear regression, with the predictors raised to a power. In a polynomial regression scenario the individual coefficients are less important compared to the overall fit of the model and the prespective it provides on the relationship between the predictors and the response.</p><p>Once a model is fit, least squares can be used to estimate the variance of each coefficient as well as the covariance between coefficient pairs.</p><p>The obtained variance estimates can be used to compute the estimated variance of <img src="https://render.githubusercontent.com/render/math?math=\hat{f}(X_0)" />. The estimated pointwise standard error of <img src="https://render.githubusercontent.com/render/math?math=\hat{f}(X_0)" /> is the square root of this variance.</p><p><strong>Step functions</strong> split the range of <em>X</em> into bins and fit a different constant to each bin. This is equivalent to converting a continuous variable into an ordered categorical variable.</p><p>First, <em>K</em> cut points, <img src="https://render.githubusercontent.com/render/math?math=c_1, c_2, ..., c_k" />, are created in the range of <em>X</em> from which <em>K+1</em> new variables are created.</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=C_{0}(X) = I(X &lt; C_{1})" />,
</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=C_{1}(X) = I(C_{2} \leq X \leq C_{3})" />,
</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=..." />,
</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=C_{K-1}(X) = I(C_{K-1} \leq X \leq C_{K})" />,
</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=C_{K-1}(X) = I(C_{K-1} \leq X \leq C_{K})" />,
</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=C_{K} = I(C_{K} \leq X)" />,
</p><p>where <em>I</em> is an indicator function that returns 1 if the condition is true.</p><p>It is worth noting that each bin is unique and</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=C_0 (X) %2B C_1 (X) %2B ... %2B C_K (X) = 1" />,
</p><p>since each variable only ends up in one of <em>K+1</em> intervals.</p><p>Once the slices have been selected, a linear model is fit using <img src="https://render.githubusercontent.com/render/math?math=c_0(X), c_1(X), ..., c_K(X)" /> as predictors:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=y_{i} = \beta_{0} %2B \beta_{1}C_{1}(X_{i}) %2B \beta_{2}C_{2}(X_{i}) %2B ... %2B \beta_{k}C_{k}(X_{i}) + \epsilon_{i}" /></p><p>Only one of <img src="https://render.githubusercontent.com/render/math?math=c_1, c_2, ..., c_K" /> can be non-xero, when <img src="https://render.githubusercontent.com/render/math?math=X \lt C" />, all the predictors will be zero. This means <img src="https://render.githubusercontent.com/render/math?math=\beta_0" /> can be interpreted as the mean value of <em>Y</em> for <img src="https://render.githubusercontent.com/render/math?math=X \lt C_1" />. Similarly, for <img src="https://render.githubusercontent.com/render/math?math=C_j \leq X \lt C_{j %2B 1}" />, the linear model reduces to <img src="https://render.githubusercontent.com/render/math?math=\beta_0 %2B \beta_j" />, so <img src="https://render.githubusercontent.com/render/math?math=\beta_j" /> represents the average increase in the response for <em>X</em> in <img src="https://render.githubusercontent.com/render/math?math=C_j \leq X \lt C_{j %2B 1}" /> compared to <img src="https://render.githubusercontent.com/render/math?math=X \lt C_1" />.</p><p>Unless there are natural breakpoints in the predictors, piecewise constant functions can miss the interesting data.</p><p><strong>Basis functions</strong>. Polynomial and piecewise constant functions are special cases of a basis function approach. The basis function approach utilises a family of functions or transformations that can be applied to a variable X: <img src="https://render.githubusercontent.com/render/math?math=b_1(X), b_2(X), ..., b_K(X)" />.</p><p>Instead of fitting a linear model in <em>X</em>, a similar model that applies the fixed and known basis functions to <em>X</em> is used:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=y_{i} = \beta_{0} %2B \beta_{1}b_{1}(x_{i}) %2B \beta_{2}b_{2}(x_{i}) %2B ... %2B \beta_{k}b_{k}(x_{i}) + \epsilon_{i}" /></p><p>For polynomial regression, the basis functions are <img src="https://render.githubusercontent.com/render/math?math=b_j(x) = x_i^j" />. For piecewise constant functions the basis functions are <img src="https://render.githubusercontent.com/render/math?math=b_j(x_i) = I(c_j \leq x_i \lt c_{j+1})" />.</p><p>Since the basis function model is just linear regression with predictors <img src="https://render.githubusercontent.com/render/math?math=b_1(x_i), b_2(x_i),..., b_k(x_i)" /> least squares can be used to estimate the unknown regression coefficients. Additionally, all the inference tools for linear models like standard error for coefficient estimates and F-statistics for overall model significance can also be employed in this setting.</p><p>Many different types of basis functions exist.</p><p><strong>Regression splines</strong>. The simplest spline is a piecewise polynomial function. Piecewise polynomial regression involves fitting separate low-degree polynomials over different regions of <em>X</em>, instead of fitting a high-degree polynomial over the entire range of <em>X</em>.
For example, a piecewise cubic polynomial is generated by fitting a cubic regression in the form</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=y_{i} = \beta_{0} %2B \beta_{1}x_{i} %2B \beta_{2}x_{i}^{2} %2B \beta_{3}x_{i}^{3} + \epsilon_{i}" /></p><p>but where the coefficients <img src="https://render.githubusercontent.com/render/math?math=\beta_0, \beta_1, \beta_2, \beta_3" />, differ in different regions  of the range of <em>X</em>. The points in the range where the coefficients change are called knots. Assuming no functions are repeated, a range of <em>X</em> split at <em>K</em> knots would be fit to <em>K+1</em> different functions of the selected type (constant, linear, cubic, etc.), one for each region.</p><p>In many situations, the number of degrees of freedom in the piecewise context can be determined by multiplying the number of parameters by one more than the number of knots. For a piecewise polynomial regression of dimension <em>d</em>, the number of degrees of freedom would be</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=d \times (K %2B 1)" /></p><p>Piecewise functions often run into the problem that they aren’t continuous at the knots. To remedy this, a constraint can be put in place that the fitted curve must be continuous. Even then the fitted curve can look unnatural.</p><p>To ensure the fitted curve is not just continuous, but also smooth, additional constraints can be placed on the derivatives of the piecewise polynomial.</p><p>A degree-d spline is a degree-d polynomial with continuity in derivatives up to degree <em>d−1</em> at each knot.</p><p>For example, a cubic spline, requires that each cubic piecewise polynomial is constrained at each knot such that the curve is continuous, the first derivative is continuous, and the second derivative is continuous. Each constraint imposed on the piecewise cubic polynomial effectively reclaims one degree of freedom by reducing complexity.</p><p>In general, a cubic spline with <em>K</em> knots uses a total of <em>4+K</em> degrees of freedom.</p><p><strong>The spline basis representation</strong> uses the basis model to represent a regression spline, for example, a cubic spline with <em>K</em> knots can be modeled as:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=y_{i} = \beta_{0} %2B \beta_{1}b_{1}(x_{i}) %2B \beta_{2}b_{2}(x_{i})%2B\ ...\ %2B \beta_{K%2B3}b_{K%2B3}(x_{i}) %2B \epsilon_{i}" /></p><p>with an appropriate choice of basis functions. Such a model could then be fit using least squares.</p><p>Though there are many ways to represent cubic splines using different choices of basis functions, the most direct way is to start off with a basis for a cubic polynomial (<img src="https://render.githubusercontent.com/render/math?math=X, X^2, X^3" />) and then add one truncated power basis function per knot. A truncated power basis function is defined as</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=h(x,\xi) = h(x - \xi)^3_{+} = \big\{ \begin{array}{cc}{(x - \xi)^3} \if x \lt \xi \\{0 \quad \quad \otherwise}\end{array}" /></p><p>where <img src="https://render.githubusercontent.com/render/math?math=\xi" /> is the knot.</p><p>It can be shown that augmenting a cubic polynomial with a term of the form <img src="https://render.githubusercontent.com/render/math?math=\beta_4 h (x, \xi)" /> will lead to discontinuity only in the third derivative of ξ. The function will remain continuous with continuous first and second derivatives at each of the knots.</p><p>The means that to fit a cubic spline to a data set with <em>K</em> knots, least squares regression can be employed with an intercept and <em>K+3</em> predictors of the form <img src="https://render.githubusercontent.com/render/math?math=X, X^2, X^3, h(X, \xi_1), h(X, \xi_2), ..., h(X, \xi_K)" /> where <img src="https://render.githubusercontent.com/render/math?math=\xi_1, \xi_2, ..., \xi_k" /> are the knots. This amounts to estimating a total of <em>K+4</em> regression coefficients and uses <em>K+4</em> degrees of freedom.</p><p>Cubic splines are popular because the discontinuity at the knots is not detectable by the human eye in most situations.</p><p>Splines can suffer from high variance at the outer range of the predictors. To combat this, a natural spline can be used. A natural spline is a regression spline with additional boundary constraints that force the function to be linear in the boundary region.</p><p>There are a variety of methods for choosing the number and location of the knots. Because the regression spline is most flexible in regions that contain a lot of knots, one option is to place more knots where the function might vary the most and fewer knots where the function might be more stable. Another common practice is to place the knots in a uniform fashion. One means of doing this is to choose the desired degrees of freedom and then use software or other heuristics to place the corresponding number of knots at uniform quantiles of the data.</p><p>Cross validation is a useful mechanism for determining the appropriate number of knots and/or degrees of freedom.</p><p>Regression splines often outperform polynomial regression. Unlike polynomials which must use a high dimension to produce a flexible fit, splines can keep the degree fixed and increase the number of knots instead. Splines can also distribute knots, and hence flexibility, to those parts of the function that most need it which tends to produce more stable estimates.</p><p><strong>Smoothing splines</strong> take a different approach to producing a spline. To fit a smooth curve to a data set, it would be ideal to find a function <em>g(X)</em> that fits the data well with a small residual sum of squares. However without any constraints on <em>g(X)</em>, it’s always possible to produce a <em>g(X)</em> that interpolates all of the data and yields an RSS of zero, but is over flexible and over fits the data. What is really wanted is a <em>g(X)</em> that makes RSS small while also remaining smooth. One way to achieve this is to find a function <em>g(X)</em> that minimizes:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\sum_{i=1}^{n}(y_{i} - g(x_{i}))^{2} + \lambda \int g''(t)^{2}dt" /></p><p>where <em>λ</em> is a non-negative tuning parameter. Such a function yields a smoothing spline. Like ridge regression and the lasso, smoothing splines utilize a loss and penalty strategy.</p><p>The term</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\lambda \int g''(t)^{2}dt" /></p><p>is a loss function that encourages <em>g</em> to be smooth and less variable. <em>g′′(t)</em> refers to the second derivative of the function <em>g</em>. The first derivative <em>g′(t)</em> measures the slope of a function at <em>t</em> and the second derivative measures the rate at which the slop is changing. Put another way, the second derivative measures the rate of change of the rate of change of <em>g(X)</em>. Roughly speaking, the second derivative is a measure of a function’s roughness. <em>g′′(t)</em> is large in absolute value if <em>g(t)</em> is very wiggly near <em>t</em> and is close to zero when <em>g(t)</em> is smooth near <em>t</em>. As an example, the second derivative of a straight line is zero because it is perfectly smooth.</p><p>The symbol <em>∫</em> indicates an integral which can be thought of as a summation over the range of <em>t</em>. All together this means that <img src="https://render.githubusercontent.com/render/math?math=\lambda \int g''(t)^{2}dt" /> is a measure of the change in <em>g′(t)</em> over its full range.</p><p>If <em>g</em> is very smooth, then <em>g′(t)</em> will be close to constant and <img src="https://render.githubusercontent.com/render/math?math=\lambda \int g''(t)^{2}dt" /> will have a small value. On the other extreme, if <em>g</em> is variable and wiggly then <em>g′(t)</em> will vary significantly and <img src="https://render.githubusercontent.com/render/math?math=\lambda \int g''(t)^{2}dt" />  will have a large value.</p><p>The tuning constant, <em>λ</em>, controls how smooth the resulting function should be. When <em>λ</em> is large, <em>g</em> will be smoother. When <em>λ</em> is zero, the penalty term will have no effect, resulting in a function that is as variable and jumpy as the training observations dictate. As <em>λ</em> approaches infinity, <em>g</em> will grow smoother and smoother until it eventually is a perfectly smooth straight line that is also the linear least squares solution since the loss function aims to minimize the residual sum of squares.</p><p>At this point it should come as no surprise that the tuning constant, <em>λ</em>, controls the bias-variance trade-off of the smoothing spline.</p><p>The smoothing spline <em>g(X)</em> has some noteworthy special properties. It is a piecewise cubic polynomial with knots at the unique values of <img src="https://render.githubusercontent.com/render/math?math=x_1, x_2, ..., x_n" />, that is continuous in its first and second derivatives at each knot. Additionally, <em>g(X)</em> is linear in the regions outside the outer most knots. Though the minimal <em>g(X)</em> is a natural cubic spline with knots at <img src="https://render.githubusercontent.com/render/math?math=x_1, x_2, ..., x_n" />, it is not the same natural cubic spline derived from the basis function approach. Instead, it’s a shrunken version of such a function where <em>λ</em> controls the amount of shrinkage.</p><p>The choice of <em>λ</em> also controls the effective degrees of freedom of the smoothing spline. It can be shown that as <em>λ</em> increases from zero to infinity, the effective degrees of freedom (dfλ) decreases from n down to 2.</p><p>Smoothing splines are considered in terms of effective degrees of freedom because though it nominally has n parameters and thus n degrees of freedom, those n parameters are heavily constrained. Because of this, effective degrees of freedom are more useful as a measure of flexibility.</p><p>The effective degrees of freedom are not guaranteed to be an integer.</p><p>The higher dfλ, the more flexible the smoothing spline. The definition of effective degrees of freedom is somewhat technical, but at a high level, effective degrees of freedom is defined as</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=df_{\lambda} = \sum_{i=1}^{n}\{S_{\lambda}\}_{ii}" /></p><p>or the sum of the diagonal elements of the matrix <img src="https://render.githubusercontent.com/render/math?math=S_{\lambda}" /> which is an n-vector of the n fitted values of the smoothing spline at each of the training points, <img src="https://render.githubusercontent.com/render/math?math=x_1, x_2, ..., x_n" />. Such an n-vector can be combined with the response vector y to determine the solution for a particular value of λ:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\hat{g}_{\lambda} = S_{\lambda}y" /></p><p>Using these values, the leave-one-out cross validation error can be calculated efficiently via</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=RSS_{cv}(\lambda) = \sum_{i=1}^{n}(y_{i} - \hat{g}_{\lambda}^{(-i)}(x_{i}))^{2} = \sum_{i=1}^{n}\frac{y_{i} - \hat{g}_{\lambda}(x_{i})}{1 - {S_\lambda}_{ii}}^{2}" /></p><p>where <img src="https://render.githubusercontent.com/render/math?math=\hat{g}^{(-1)}_{\lambda}" /> refers to the fitted value using all training observations except for the ith.</p><p>**Local regression ** is an approach to fitting flexible non-linear functions which involves computing the fit at a target point <img src="https://render.githubusercontent.com/render/math?math=x_0" /> using only the nearby training observations.</p><p>Each new point from which a local regression fit is calculated requires fitting a new weighted least squares regression model by minimizing the appropriate regression weighting function for a new set of weights.</p><p>A general algorithm for local regression is</p><ol><li>Select the fraction <img src="https://render.githubusercontent.com/render/math?math=s=\frac{k}{n}" /> of training points whose <img src="https://render.githubusercontent.com/render/math?math=x_i" /> are closest to <img src="https://render.githubusercontent.com/render/math?math=x_0" />.</li><li>Assign a weight <img src="https://render.githubusercontent.com/render/math?math=K_{i0}=K(xi,x0)" /> to each point in this neighborhood such that the point furthest from <img src="https://render.githubusercontent.com/render/math?math=x_0" /> has a weight of zero and the point closest to <img src="https://render.githubusercontent.com/render/math?math=x_0" /> has the highest weight. All but the <em>k</em> nearest neighbors get a weight of zero.</li><li>Fit a weighted least squares regression of the <img src="https://render.githubusercontent.com/render/math?math=y_i" /> on to the <img src="https://render.githubusercontent.com/render/math?math=x_i" /> using the weights calculated earlier by finding coefficients that minimize a modified version of the appropriate least squares model. For linear regression that modified model is</li></ol><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\sum_{i=1}^{n} K_{i0}(y_{i} - \beta_{0} - \beta_{1}x_{i})^{2}" /></p>
4. The fitted value at <img src="https://render.githubusercontent.com/render/math?math=x_0" /> is given by
<p align="center"><img src="https://render.githubusercontent.com/render/math?math=\hat{f}(x_{0}) = \hat{\beta}_{0} + \hat{\beta}_{1}x_{0}" /></p>
Local regression is sometimes referred to as a memory-based procedure because the whole training data set is required to make each prediction.
<p>In order to perform local regression, a number of important choices must be made.</p><ul><li>How should the weighting function K be defined?</li><li>What type of regression model should be used to calculate the weighted least squares? Constant, linear, quadratic?</li><li>What size should be given to the span S?</li></ul><p>The most important decision is the size of the span <em>S</em>. The span plays a role like <em>λ</em> did for smoothing splines, offering some choice with regard to the bias-variance trade-off. The smaller the span <em>S</em>, the more local, flexible, and wiggly the resulting non-linear fit will be. Conversely, a larger value of <em>S</em> will lead to a more global fit. Again, cross validation is useful for choosing an appropriate value for <em>S</em>.</p><p>In the multiple linear regression setting, local regression can be generalized to yield a multiple linear regression model in which some variable coefficients are globally static while other variable coefficients are localized. These types of varying coefficient models are a useful way of adapting a model to the most recently gathered data.</p><p>Local regression can also be useful in the multi-dimensional space though the curse of dimensionality limits its effectiveness to just a few variables.</p><p><strong>Generalized additative models (GAMs)</strong> offer a general framework for extending a standard linear model by allowing non-linear functions of each of the predictors while maintaining additivity. GAMs can be applied with both quantitative and qualitative models.</p><p>One way to extend the multiple linear regression model</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=y_{i} = \beta_{0} %2B \beta_{1}x_{i1} %2B \beta_{2}x_{i2} %2B\ ...\ %2B\beta_{p}x_{ip} %2B \epsilon_{i}" /></p><p>to allow for non-linear relationships between each feature and the response is to replace each linear component, <img src="https://render.githubusercontent.com/render/math?math=\beta_jx_{ij}" />, with a smooth non-linear function <img src="https://render.githubusercontent.com/render/math?math=f_j(x_{ij})" />
, which would yield the model</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=y_{i} = \beta_{0} %2B \beta_{1}f_{1}(x_{i1}) %2B \beta_{2}f_{2}(x_{i2}) %2B\ ...\ %2B \beta_{p}f_{p}(x_{ip}) %2B \epsilon_{i} = \beta_{0} %2B \sum_{j=1}^{p} f_{j}(x_{ij}) %2B \epsilon_{i}" /></p><p>This model is additive because a separate fj is calculated for each xi and then added together.</p><p>The additive nature of GAMs makes them more interpretable than some other types of models.</p><p>GAMs allow for using the many methods of fitting functions to single variables as building blocks for fitting an additive model.</p><p>Backfitting can be used to fit GAMs in situations where least squares cannot be used. Backfitting fits a model involving multiple parameters by repeatedly updating the fit for each predictor in turn, hold the others fixed. This approach has the benefit that each time a function is updated the fitting method for a variable can be applied to a partial residual.</p><p>A partial residual is the remainder left over after subtracting the products of the fixed variables and their respective coefficients from the response. This residual can be used as a response in a non-linear regression of the variables being updated.</p><p>For example, given a model of</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=y_{i} = f_{1}(x_{i1}) %2B f_{2}(x_{i2}) %2B f_{3}(x_{i3})" /></p><p>a residual for <img src="https://render.githubusercontent.com/render/math?math=x_{i3}" /> could be computed as</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=r_{i} = y_{i} - f_1(x_{i1}) %2B f_{2}(x_{i2})." /></p><p>The yielded residual can then be used as a response in order to fit <img src="https://render.githubusercontent.com/render/math?math=f_3" /> in a non linear regression on <img src="https://render.githubusercontent.com/render/math?math=x_3" />.</p><ul><li>GAMs allow fitting non-linear functions for each variable simultaneously, allowing for non-linearity while also avoiding the cost of trying many different transformations on each variable individually.</li><li>The additive model makes it possible to consider each <img src="https://render.githubusercontent.com/render/math?math=x_j" /> on <em>y</em> individually while holding other variables fixed. This makes inference more possible. Each function fj for the variable xij can be summarized in terms of degrees of freedom.</li><li>The additivity of GAMs also turns out to be their biggest limitation, since with many variables important interactions can be obscured. However, like linear regression, it is possible to manually add interaction terms to the GAM model by adding predictors of the form <em>xj×xk</em>. In addition, it is possible to add low-dimensional interaction terms  that can be fit using two dimensional smoothers like local regression or using two-dimensional splines.</li><li>Overall, GAMs provide a useful compromise between linear and fully non-parametric models.</li></ul><h2 id="tree-based-methods">Tree-Based Methods:</h2><p>Tree-based methods, also known as decision tree methods, involve stratifying or segmenting the predictor space into a number of simple regions. Predictions are then made using the mean or the mode of the training observations in the region to which the predictions belong. These methods are referred to as trees because the splitting rules used to segment the predictor space can be summarized in a tree.</p><p>Though tree-based methods are simple and useful for interpretation, they typically aren’t competitive with the best supervised learning techniques. Because of this, approaches such as bagging, random forests, and boosting have been developed to produce multiple trees which are then combined to yield a since consensus prediction. Combining a large number of trees can often improve prediction accuracy at the cost of some loss in interpretation.</p><p><strong>Regression Trees</strong> are decision trees they are typically drawn upside down with the leaves or terminal nodes at the bottom of the tree. The points in the tree where the predictor space is split are referred to as internal nodes. The segments of the tree that connect nodes are referred to as branches.</p><p>Regression trees are calculated in two steps:</p><ol><li>Divide the predictor space, <img src="https://render.githubusercontent.com/render/math?math=x_1, x_2, x_3, ..., x_p" /> into <em>J</em> distinct and non-overlapping regions, <img src="https://render.githubusercontent.com/render/math?math=R_1, R_2, ..., R_J" />.</li><li>For every observation that falls into the region <img src="https://render.githubusercontent.com/render/math?math=R_j" />, make the same prediction, which is the mean value of the response values for the training observations in <img src="https://render.githubusercontent.com/render/math?math=R_j" />.</li></ol><p>To determine the appropriate <img src="https://render.githubusercontent.com/render/math?math=R_1, R_2, ..., R_J" />, it is preferable to divide the predictor space into high-dimensional rectangles, or boxes for simplicity and ease of interpretation. Ideally the goal would be to find regions that minimize the residual sum of squares given by</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\sum_{j=1}^{J}\sum_{i \in R_{j}}(y_{i} - \hat{y}_{R_{j}})^{2}" /></p><p>where <img src="https://render.githubusercontent.com/render/math?math=\hat{y}_{R_{j}}" /> is the mean response for the training observations in the jth box. It is computationally infeasible to consider every possible partition of the feature space into J boxes. For this reason, a top-down, greedy approach known as recursive binary splitting is used.</p><p>Recursive binary splitting is considered top-down because it begins at the top of the tree where all observations belong to a single region, and successively splits the predictor space into two new branches. Recursive binary splitting is greedy because at each step in the process the best split is made relative to that particular step rather than looking ahead and picking a split that will result in a better split at some future step.</p><p>At each step the predictor <img src="https://render.githubusercontent.com/render/math?math=X_j" />and the cutpoint <em>s</em> are selected such that splitting the predictor space into regions <img src="https://render.githubusercontent.com/render/math?math=\{X|X_j \lt s\}" /> and <img src="https://render.githubusercontent.com/render/math?math=\{X|X_j \geq s\}" /> leads to the greatest possible reduction in the residual sum of squares. This means that at each step, all the predictors <img src="https://render.githubusercontent.com/render/math?math=X_1, X_2, ..., X_j" /> and all possible values of the cutpoint <em>s</em> for each of the predictors are considered. The optimal predictor and cutpoint are selected such that the resulting tree has the lowest residual sum of squares compared to the other candidate predictors and cutpoints.</p><p>More specifically, for any <em>j</em> and <em>s</em> that define the half planes <img src="https://render.githubusercontent.com/render/math?math=R_1(j, s) = \{X|X_j \lt s\}" /> and, <img src="https://render.githubusercontent.com/render/math?math=R_2(j, s) = \{X|X_j \geq s\}" />. The objective is to find the <em>j</em> and <em>s</em> that minimise the equation,</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\sum_{i: x_{i} \in R_{1}(j, s)}(y_{i} - \hat{y}_{R_{1}})^{2} %2B \sum_{i: x_{i} \in R_{2}(j, s)}(y_{i} - \hat{y}_{R_{2}})^{2}" /></p><p>where <img src="https://render.githubusercontent.com/render/math?math=\hat{y}_{R_{1}}" /> and <img src="https://render.githubusercontent.com/render/math?math=\hat{y}_{R_{2}}" /> are teh mean responses for the training observations in the respective regions. Only one region is split each iteration, the process concludes when some halting criteria are met.</p><p><strong>Tree pruning</strong> is the process of reducing the complexity of the fitted trees. A smaller tree often leads to lower variance and better interpretation at the cost of a little bias.</p><p>One option might be to predicate the halting condition on the reduction in the residual sum of squares, but this tends to be short sighted since a small reduction in RSS might be followed by a more dramatic reduction in RSS in a later iteration.</p><p>Because of this it is often more fruitful to grow a large tree, <img src="https://render.githubusercontent.com/render/math?math=T_0" />, and then prune it down to a more optimal subtree.</p><p>Ideally, the selected subtree should have the lowest test error rate. However, this is impractical in practice since there a huge number of possible subtrees. Computationally it would be preferable to select a small set of subtrees for consideration.</p><p>Cost complexity pruning, also known as weakest link pruning, reduces the possibility space to a sequence of trees indexed by a non-negative tuning parameter, α.
For each value of α there corresponds a subtree, <img src="https://render.githubusercontent.com/render/math?math=T \subset T_{0}" />, such that</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\sum_{m=1}^{|T|}\sum_{i:X_{i} \in R_{m}}(y_{i} - \hat{y}_{R_{m}})^{2} %2B \alpha|T|" /></p><p>where <em>|T|</em> indicates the number of terminal nodes in the tree <em>T</em>, <img src="https://render.githubusercontent.com/render/math?math=R_m" /> is the predictor region corresponding to the mth terminal node and <img src="https://render.githubusercontent.com/render/math?math=\hat{y}_{R_{m}}" /> is the predicted response associated with <img src="https://render.githubusercontent.com/render/math?math=R_{m}" /> (the mean of the training observations in <img src="https://render.githubusercontent.com/render/math?math=R_{m}" />).</p><p>The tuning parameter α acts as a control on the trade-off between the subtree’s complexity and its fit to the training data. When α is zero, then the subtree will equal <img src="https://render.githubusercontent.com/render/math?math=T_0" /> since the training fit is unaltered. As α increases, the penalty for having more terminal nodes increases, resulting in a smaller subtree.</p><p>As α increases from zero, the pruning process proceeds in a nested and predictable manner which makes it possible to obtain the whole sequence of subtrees as a function of α easily.</p><p>A validation set or cross-validation can be used to select a value of α. The selected value can then be used on the full training data set to produce a subtree corresponding to α. This process is summarized below.</p><ol><li>Use recursive binary splitting to grow a large tree from the training data, stopping only when each terminal node has fewer than some minimum number of observations.</li><li>Apply cost complexity pruning to the large tree to obtain a sequence of best subtrees as a function of α.</li><li>Use k-fold cross validation to choose α. Divide the training observations into k folds. For each k=1, k=2, ..., k=K:
<ol><li>Repeat steps 1 and 2 on all but the kth fold of the training data.</li><li>Evaluate the mean squared prediction error on the data in the left-out kth fold as a function of α. Average the results for each value of α and pick α to minimize the average error.</li></ol></li><li>Return the subtree from step 2 that corresponds to the chosen value of α.</li></ol><p><strong>Classification trees</strong> are similar to regression trees, however they are used to predict qualitative responses. For a classification tree, predictions are made based on the notion that each observation belongs to the most commonly occurring class of the training observations in the region to which the observation belongs.</p><p>In interpreting a classification tree, the class proportions within a particular terminal node region are often also of interest.</p><p>Growing a classification tree is similar to growing a regression tree, however residual sum of squares cannot be used as a criteria for making binary splits and instead classification error rate is used. Classification error rate for a given region is defined as the fraction of training observations in that region that do not belong to the most common class. Formally, <img src="https://render.githubusercontent.com/render/math?math=E = 1 - {max}_{k}(\hat{p}_{mk})" /> where <img src="https://render.githubusercontent.com/render/math?math=\hat{p}_{mk}" /> represents the proportion of the training observations in the mth region that are from the kth class.</p><p>In practice, it turns out that classification error rate is not sensitive enough for tree growing and other criteria are preferable. The Gini index is a measure of the total variance across the K classes defined as</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=G = \sum_{k=1}^{K} \hat{p}_{mk}(1-\hat{p}_{mk})" /></p><p>where again <img src="https://render.githubusercontent.com/render/math?math=\hat{p}_{mk}" /> represents the proprtion of the training obsaervations in th8e mth region that are fromthe kth class.</p><p>The Gini index can be viewed as a measure of region purity as a small value indicates the region contains mostly observations from a single class.</p><p>An alternative to the Gini index is cross-entropy, defined as</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=D = \sum_{k=1}^{K}\hat{p}_{mk} \mathrm{log}\ \hat{p}_{mk}" /></p><p>Since <img src="https://render.githubusercontent.com/render/math?math=\hat{p}_{mk}" /> must always be betweeen zero and oen it reasons that
<img src="https://render.githubusercontent.com/render/math?math=\hat{p}_{mk} \mathrm{log}\ \hat{p}_{mk} \geq 0" />. Like the Gini index, cross-entropy will take on a small value if the mth region is pure.</p><p>When growing a tree, the Gini index or cross-entropy are typically used to evaluate the quality of a particular split since both methods are more sensitive to node purity than classification error rate is.</p><p>When pruning a classification tree, any of the three measures can be used, though classification error rate tends to be the preferred method if the goal of the pruned tree is prediction accuracy.</p><p>Compared to linear models, decision trees will tend to do better in scenarios where the relationship between the response and the predictors is non-linear and complex. In scenarios where the relationship is well approximated by a linear model, an approach such as linear regression will tend to better exploit the linear structure and outperform decision trees.</p><p><strong>Advantages of Trees</strong></p><ul><li>Trees are easy to explain; even easier than linear regression.</li><li>Trees, arguably, more closely mirror human decision-making processes than regression or classification.</li><li>Trees can be displayed graphically and are easily interpreted by non-experts.</li><li>Trees don’t require dummy variables to model qualitative variables.</li></ul><p><strong>Disadvantages of Trees</strong></p><ul><li>Interpretability comes at a cost; trees don’t typically have the same predictive accuracy as other regression and classification methods.</li><li>Trees can lack robustness. Small changes in the data can cause large changes in the final estimated tree.</li></ul><p>Aggregating many decision trees using methods such as bagging, random forests, and boosting can substantially improve predictive performance and help mitigate some of the disadvantages of decision trees.</p><p><strong>Bootstrap aggregation, or bagging</strong>, is a general purpose procedure for reducing the variance of statistical learning methods that is particularly useful for decision trees.</p><p>Given a set of n independent observations, <img src="https://render.githubusercontent.com/render/math?math=Z_1, Z_2, ..., Z_n" />, each with a variance of <img src="https://render.githubusercontent.com/render/math?math=\sigma^2" />, the variance of the mean <img src="https://render.githubusercontent.com/render/math?math=\bar{Z}" /> of the observations is given by <img src="https://render.githubusercontent.com/render/math?math=\frac{\sigma^2}{n}" />. In simple terms, averaging a set of observations reduces variance. This means that taking many training sets from the population, building a separate predictive model from each, and then averaging the predictions of each model can reduce variance.</p><p>More formally, bagging aims to reduce variance by calculating <img src="https://render.githubusercontent.com/render/math?math=\hat{f}^{*1}(x), \hat{f}^{*2}, ..., \hat{f}^{*B}" /> using <em>B</em> separate training sets created using bootstrap resampling, and averaging the results of the functions to obtain a single, low-variance statistical learning model given by</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\hat{f}_{avg}(x) = \frac{1}{B}\sum_{b=1}^{B}\hat{f}^{*b}(x)" /></p><p>Bagging can improve predictions for many regression methods, but it’s especially useful for decision trees. Bagging is applied to regression trees by constructing <em>B</em> regression trees using <em>B</em> bootstrapped training sets and averaging the resulting predictions. The constructed trees are grown deep and are not pruned. This means each individual tree has high variance, but low bias. Averaging the results of the <em>B</em> trees reduces the variance.</p><p>In the case of classification trees, a similar approach can be taken, however instead of averaging the predictions, the prediction is determined by the most commonly occurring class among the <em>B</em> predictions or the mode value.</p><p>The number of trees, <em>B</em>, is not a critical parameter with bagging. Picking a large value for <em>B</em> will not lead to overfitting. Typically, a value of <em>B</em> is chosen to ensure the variance and error rate of settled down.</p><p><strong>Out of bag error estimation.</strong> It can be shown that, on average, each bagging tree makes use of around two-thirds of the training observations. The remaining third of the observations that are not used to fit a given bagged tree are referred to as the out-of-bag observations. An approximation of the test error of the bagged model can be obtained by taking each of the out-of-bag observations, evaluating the B/3 predictions from those trees that did not use the given out-of-bag prediction, taking the mean/mode of those predictions, and comparing it to the value predicted by the bagged model, yielding the out-of-bag error. When <em>B</em> is sufficiently large, out-of-bag error is nearly equivalent to leave-one-out cross validation.</p><p><strong>Variable Importance Measures</strong> Bagging improves prediction accuracy at the expense of interpretability. Averaging the amount that the residual sum of squares decreased for a given predictor over all <em>B</em> trees can be useful as a metric on the importance of the given predictor. Similarly, the decrease in the Gini index can be used as a metric on the importance of the given predictor for classification models.</p><p><strong>Random forests</strong> are similar to bagged trees, however, random forests introduce a randomised process that helps decorrelate trees.</p><p>During the random forest tree construction process, each time a split in a tree is considered, a random sample of <em>m</em> predictors is chosen from the full set of <em>p</em> predictors to be used as candidates for making the split. Only the randomly selected <em>m</em> predictors can be considered for splitting the tree in that iteration. A fresh sample of <em>m</em> predictors is considered at each split. Typically <img src="https://render.githubusercontent.com/render/math?math=m \approx \sqrt{p}" /> meaning that the number of predictors considered at each split is approximately equal to the square root of the total number of predictors, <em>p</em>. This means that at each split only a minority of the available predictors are considered. This process helps mitigate the strength of very strong predictors, allowing more variation in the bagged trees, which ultimately helps reduce correlation between trees and better reduces variance. In the presence of an overly strong predictor, bagging may not outperform a single tree. A random forest would tend to do better in such a scenario.</p><p>On average, <img src="https://render.githubusercontent.com/render/math?math=\frac{p-m}{p}" /> of the splits in a random forest will not even consider the strong predictor which causes the resulting trees to be less correlated. This process is a kind of decorrelation process.</p><p>As with bagging, random forests will not overfit as <em>B</em> is increased, so a value of <em>B</em> should be selected that allows the error rate to settle down.</p><p><strong>Boosting</strong> works similarly to bagging, however, where as bagging builds each tree independent of the other trees, boosting trees are grown using information from previously grown trees. Boosting also differs in that it does not use bootstrap sampling. Instead, each tree is fit to a modified version of the original data set. Like bagging, boosting combines a large number of decision trees, <img src="https://render.githubusercontent.com/render/math?math=\hat{f}^{*1}, \hat{f}^{*2}, ..., \hat{f}^{*B}" />.</p><p>Each new tree added to a boosting model is fit to the residuals of the model instead of the response, <em>Y</em>.</p><p>Each new decision tree is then added to the fitted function to update the residuals. Each of the trees can be small with just a few terminal nodes, determined by the tuning parameter, <em>d</em>.</p><p>By repeatedly adding small trees based on the residuals, <img src="https://render.githubusercontent.com/render/math?math=\hat{f}" /> will slowly improve in areas where it does not perform well.</p><p>Boosting has three tuning parameters:</p><ul><li><em>B</em>, the number of trees. Unlike bagging and random forests, boosting can overfit if <em>B</em> is too large, although overfitting tends to occur slowly if at all. Cross validation can be used to select a value for <em>B</em>.</li><li>λ, the shrinkage parameter, a small positive number that controls the rate at which the boosting model learns. Typical values are 0.01 or 0.001, depending on the problem. Very small values of λ can require a very large value for <em>B</em> in order to achieve good performance.</li><li><em>d</em>, the number of splits in each tree, which controls the complexity of the boosted model. Often d=1 works well, in which case each tree is a stump consisting of one split. This approach yields an additive model since each involves only a single variable. In general terms, <em>d</em> is the interaction depth of the model and it controls the interaction order of the model since <em>d</em> splits can involve at most <em>d</em> variables.</li></ul><p>An algorithm for boosting regression trees:</p><ol><li>Set <img src="https://render.githubusercontent.com/render/math?math=\hat{f}(x) = 0" /> and &lt;"img src="https://render.githubusercontent.com/render/math?math=r_i = y"&gt; for all <em>i</em> in the training set.</li><li>For <em>b = 1, b = 2, ..., b = B</em>, repeat:
<ol><li>Fit a tree <img src="https://render.githubusercontent.com/render/math?math=\hat{f}^b" />  with <em>d</em> splits to the training data (X, r)</li><li>Update <img src="https://render.githubusercontent.com/render/math?math=\hat{f}^b" /> by adding a shrunken version of the new tree:</li></ol><img src="https://render.githubusercontent.com/render/math?math=\hat{f}(x) \Leftarrow \hat{f}(x) %2B \lambda\hat{f}^{b}(x) " />
 3. Update the residuals
 <img src="https://render.githubusercontent.com/render/math?math=r_i \Leftarrow r_i - \lambda \hat{f}^b (x_i) " /></li><li>Output the boosted model,</li></ol><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\hat{f}(x) = \sum_{b=1}^{B}\lambda\hat{f}^{b}(x)" /></p><h2 id="support-vector-machines">Support Vector Machines:</h2><p>Support vector machines (SVMs) are a generalisation of a simple and intuitive classifier called the maximal margin classifier. Support vector machines improve upon maximal margin classifiers by using a support vector classifier which overcomes a limitation of the maximal margin classifier which requires that classes must be separable by a linear boundary. The use of the support vector classifier allows support vector machines to be applied to a wider range of cases than the maximal margin classifier. Support vector machines extend the support vector classifier to accommodate non-linear class boundaries.</p><p>Support vector machines are intended for the binary classification setting in which there are two classes, but can be extended to handle more than two classes.</p><p>In a <em>p-dimensional</em> space, a hyperplane is a flat affine subspace of dimension <em>p−1</em>. For example, in two dimensions, a hyperplane is a flat one-dimensional subspace, or in other words, a line. In three dimensions, a hyperplane is a plane. The word affine indicates that the subspace need not pass through the origin.</p><p>A p-dimensional hyperplane is defined as</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\beta_0 %2B \beta_1X_1 %2B \beta_2X_2 %2B ... %2B \beta_pX_p = 0" /></p><p>which means that any <img src="https://render.githubusercontent.com/render/math?math=X = (X_1, X_2, ..., X_p)^T" /> for which the above hyperplane equation holds is a point on the hyperplane.</p><p>If <img src="https://render.githubusercontent.com/render/math?math=X = (X_1, X_2, ..., X_p)^T" /> doesn’t fall on the hyperplane, then it must fall on one side of the hyperplane or the other. As such, a hyperplane can be thought of as dividing a p-dimensional space into two partitions. Which side of the hyperplane a point falls on can be computed by calculating the sign of the result of plugging the point into the hyperplane equation.</p><p><strong>Classification with a separating hyperplane</strong>. Consider an <em>n by p</em> matrix <em>X</em> that consists of <em>n</em> training observations in p-dimensional space.</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\normalsize X_{1} = \left\lgroup \begin{array}{cc} X_{11}\\ \vdots\\ X_{1p} \end{array} \right\rgroup ,..., X_{n} = \left\lgroup \begin{array}{cc} X_{n1}\\ \vdots\\ X_{np} \end{array} \right\rgroup" /></p><p>where each of these observations fall into two classes, or <img src="https://render.githubusercontent.com/render/math?math=y_1, ..., y_n \in -1, 1" /> where −1 represents one class and 1 represents the other. Also available is a test observation constituted of a p-vector of observed features, <img src="https://render.githubusercontent.com/render/math?math=x^* = (x^*_1, x^*_2, ..., x^*_p)" />.</p><p>Suppose that it is possible to construct a hyperplane that separates the training observations perfectly according to their class labels.</p><p>Such a hyperplane would have a property that</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\beta_{0} + \beta_{1}X_{1} + \beta_{2}X_{2} +\ ...\ %2B \beta_{p}X_{p} &lt; 0\ \mathrm{if}\ y_{i} = -1" /></p><p>and</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\beta_{0} + \beta_{1}X_{1} + \beta_{2}X_{2} +\ ...\ %2B \beta_{p}X_{p} &gt; 0\ \mathrm{if}\ y_{i} = 1" /></p><p>More concisely, a separating hyperplane would have the property that</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=y_i(\beta_{0} + \beta_{1}X_{1} + \beta_{2}X_{2} +\ ...\ %2B \beta_{p}X_{p}) &gt; 0\ \mathrm{for all}\ i = 1, .., n" /></p><p>If a separating hyperplane exists it can be used to construct a classifier that assigns a test observation to a class depending on which side of the hyperplane the observation is located.</p><p>That is, a test observation is classified based on the sign of</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=f(x^{*}) = \beta_{0} %2B \beta_{1}x_{1}^{*} %2B \beta_{2}x_{2}^{*} %2B ... %2B \beta_{p}x_{p}^{*}" /></p><p>If f(x∗) is positive then the test observation is assigned to class 1. If f(x∗) is negative then the test observation is assigned to class -1. Additionally, the magnitude of f(x∗) can be used as a measure of confidence in the class assignment for x∗. If f(x∗) is far from zero, it is far from the hyperplane and more confidence in the classification can be had. Conversely, If f(x∗) is near to zero then the classification is less certain.</p><p>A classifier based on a separating hyperplane has a linear decision boundary.</p><p><strong>The maximal margin classifier</strong> is an approach to selecting a hyperplane classifier from the inifinite possible separating hyperplanes. If the data can be perfectly separated using a hyperplane, then there will in fact exist an infnite nubmer of such hyperplanes.</p><p>The maximal margin (optimal) separating hyperplane is the hyperplane that which is farthest from the training observations in terms of perpendicular distance. The minimal distance from the observations to the hyperplane is known as the margin. The maximal margin hyperplane is the hyperplane that has the farthest minimum distance to the training observations. When a maximal margin hyperplane is used to classify test observations it is known as a maximal margin classifier. The maximal margin classifier is often successful, but can overfit when <em>p</em> is large.</p><p>The maximal margin classifier classifies a test observation <img src="https://render.githubusercontent.com/render/math?math=x^*" /> based on the sign of</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=f(x^{*}) = \beta_{0} %2B \beta_{1}x_{1}^{*} %2B \beta_{2}x_{2}^{*} %2B ... %2B \beta_{p}x_{p}^{*}" /></p><p>where <img src="https://render.githubusercontent.com/render/math?math=\beta_{0} %2B \beta_{1} %2B ... %2B \beta_{p}" /> are the coefficients of the maximal margin hyperplane.</p><p>The maximal margin hyperplane represents the mid-line of the widest gap between the two classes.</p><p>Though it might seem that the maximal margin hyperplane depends on all of the training observations, there are actually relatively few training observations that affect the maximal margin hyperplane. Those observations which are constituted of p-dimensional vectors, are those observations that would cause the maximal margin hyperplane to move if they were moved in some dimension. These observations are known as support vectors since, in a sense, they support the maximal margin hyperplane and give it its shape.</p><p>The maximal margin hyperplane is the solution to the optimization problem of maximizing
<img src="https://render.githubusercontent.com/render/math?math=M_{\beta_0, \beta_1, ..., \beta_p}" /> such that</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\sum^{p}_{j=1} \beta^2_j = 1" /></p><p>and</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=y_{i}(\beta_{0} %2B \beta_{1}x_{i1} %2B \beta_{2}x_{i2} %2B\ \dots\ %2B \beta_{p}x_{ip}) \geq M\ \forall\ i=1,\ i=2,\ \dots,\ i=n" /></p><p>This constraint guarantees each ovserviation will be on the correct side of the hyperplane, assuming <em>M</em> is positive. Ensuring each observation is on the correct side of the hyperplane actually entails a slooser constraint:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=y_{i}(\beta_{0} %2B \beta_{1}x_{i1} %2B \beta_{2}x_{i2} %2B\ \dots\ %2B \beta_{p}x_{ip}) &gt; 0" /></p><p>however, the maximal margin hyperplane aims to maximize the cushion between the hyperplane and the observations.</p><p>It is worth noting that</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=y_{i}(\beta_{0} %2B \beta_{1}x_{i1} %2B \beta_{2}x_{i2} %2B\ \dots\ %2B \beta_{p}x_{ip})" /></p><p>doesn't really constrain the hyperplane since if <img src="https://render.githubusercontent.com/render/math?math=(\beta_{0} %2B \beta_{1}x_{i1} %2B \beta_{2}x_{i2} %2B\ \dots\ %2B \beta_{p}x_{ip} = 0" /> defines a hyperplane, then so does the same formula multiplied by a non-zero scalar value. But, combined with the constraint that</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\sum^{p}_{j=1} \beta^2_j = 1" /></p><p>the constraints ensure that the perpendicular distance from the ith observation to the hyperplane is given by <img src="https://render.githubusercontent.com/render/math?math=y_i(\beta_{0} %2B \beta_{1}x_{i1} %2B \beta_{2}x_{i2} %2B\ \dots\ %2B \beta_{p}x_{ip})" />.</p><p>All together, the optimization problem chooses coefficients to maximize <em>M</em> while ensuring that each observation is on the right side of the hyperplane and at least distance <em>M</em> from the hyperplane, yielding a maximal margin hyperplane.</p><p>If no separating hyperplane exists, no maximal margin hyperplane exists either. However, a soft margin can be used to construct a hyperplane that almost separates the classes. This generalisation of the maximal margin classifier is known as the support vector classifier.</p><p>In general, a perfectly separating hyperplane can be undesirable because it can be very sensitive to individual observations. This sensitivity can also be an indication of overfitting.</p><p>A classifier based on a hyperplane that doesn’t perfectly separate the two classes can offer greater robustness to variations in individual observations and better classification of most training observations at the cost of misclassifying a few training observations.</p><p>The support vector classifier, sometimes called a soft margin classifier, allows some observations to fall on both the wrong side of the margin and the wrong side of the hyperplane.</p><p>This flexibility allows the support vector classifier to utilize a hyperplane that solves the optimization problem of maximizing <img src="https://render.githubusercontent.com/render/math?math=M_{\beta_0, \beta_1, ..., \beta_p}" /> subject to</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\sum^{p}_{j=1} \beta^2_j = 1" /></p><p>and</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=y_{i}(\beta_{0} %2B \beta_{1}x_{i1} %2B \beta_{2}x_{i2} %2B\ \dots\ %2B \beta_{p}x_{ip}) &gt; M(1- \epsilon_i)" /></p><p>where <img src="https://render.githubusercontent.com/render/math?math=\epsilon \geq 0" /> and <img src="https://render.githubusercontent.com/render/math?math=\sum_{i=1}^n \epsilon_i \geq C" /> where <em>C</em> is a non-negative tuning parameter.</p><p>Like the maximal margin classifier, <em>M</em> is the width of the margin and the focus of the optimization. <img src="https://render.githubusercontent.com/render/math?math=\epsilon_1, ..., \epsilon_M" /> are slack variables that allow individual variables to fall on the wrong side of the margin and/or hyperplane. As with the maximal margin classifier, once an optimal solution has been found, a test observation can be classified based on the sign of</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=f(x^{*}) = \beta_{0} %2B \beta_{1}x_{1}^{*} %2B \beta_{2}x_{2}^{*} %2B\dots\ %2B \beta_{p}x_{p}^{*}" /></p><p>The slack variables, <img src="https://render.githubusercontent.com/render/math?math=\epsilon_1, ..., \epsilon_n" /> encode the location of the ith observation relative to the hyperplane and relative to the margin. When <img src="https://render.githubusercontent.com/render/math?math=\epsilon_i = 0" />, the ith observation is on the correct side of the margin. When <img src="https://render.githubusercontent.com/render/math?math=\epsilon_i" /> is greater than zero, the ith observation is on the wrong side of the margin and is said to have violated the margin. When <img src="https://render.githubusercontent.com/render/math?math=\epsilon_i" /> is greater than 1, the observation is on the wrong side of the hyperplane.</p><p>The tuning parameter <em>C</em> limits the sum of the slack variables, <img src="https://render.githubusercontent.com/render/math?math=\epsilon_1, ..., \epsilon_n" />, and so determines the number and severity of the violations to the margin and hyperplane that will be tolerated. When <em>C</em> is zero, no budget is available for violations, which means <img src="https://render.githubusercontent.com/render/math?math=\epsilon_i =0, ..., \epsilon_n =0" />, in which case the solution, if one exists, is the same as the maximal margin classifier. When <em>C</em> is greater than zero, no more than <em>C</em> observations may be on the wrong side of the hyperplane since <img src="https://render.githubusercontent.com/render/math?math=\epsilon_i" /> will be greater than zero and
<img src="https://render.githubusercontent.com/render/math?math=\sum^n_i \epsilon_i \leq C" />. As <em>C</em>) increases, the margin gets wider and more tolerant of violation. As <em>C</em> decreases, the margin gets narrower. Like other tuning parameters, <em>C</em>, is generally selected using cross validation.</p><p>Similar to other tuning parameters, <em>C</em> also controls the bias-variance trade-off of the statistical learning model. When <em>C</em> is small, margins will be narrow and rarely violated which amounts to a classifier that is highly fit to the data with low bias, but high variance. Conversely, when <em>C</em> is large, the margin is large and more frequently violated which amounts to a classifier that is more loosely fit with potentially higher bias, and potentially lower variance. Interestingly, only the observations that lie on the margin or violate the margin affect the hyperplane and classifier obtained. Observations directly on the margin or on the wrong side of the margin for their class are known as support vectors since the observations do affect the shape of the support vector classifier. This behavior is the reason why the tuning parameter C controls the bias-variance trade-off of the support vector classifier. When <em>C</em> is large, the margin is wide and more frequently violated which means many support vectors contribute to shaping the hyperplane. This results in low variance but potentially high bias. Conversely, when C is small there will be fewer support vectors and the resulting classifier will have low bias, but high variance.</p><p>Because the support vector classifier is based on only a small subset of the training observations (the support vectors), it is robust to the behavior of those observations that are far from the hyperplane.</p><p>When class boundaries are non-linear, the feature space can be enlarged using functions of the predictors such as quadratic, cubic, or interaction terms to address the non-linearity. However, computations can become unmanageable in the support vector classifier case and because of this, support vector machines were introduced to allow for enlarging the feature space used by the support vector classifier in a way that leads to efficient calculations.</p><p>The support vector machine is an extension of the support vector classifier that results from enlarging the feature space using kernels to accommodate a non-linear boundary between classes.</p><p>The exact details of how the support vector classifier is computed are somewhat technical, but it turns out that the solution to the support vector classifier involves only the inner-products of the observations, not the observations themselves. The inner-product of two p-vectors <em>a</em> and <em>b</em> is defined as <img src="https://render.githubusercontent.com/render/math?math=\langle a, b\rangle = \sum_{i=1}^{r} a_{i}b_{i}" />. As such, the inner product of two observations xi and xi′ is given by</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\langle x_{i}, x_{i^{\prime}} \rangle = \sum_{j=1}^{p} x_{ij}x_{i^{\prime} j}" /></p><p>It can be shown that the linear support vector can be represented as</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=f(x) = \beta_0 %2B \sum \alpha_i\ \langle x_{ij}x_{i^{\prime} j} \rangle" /></p><p>where there are <img src="https://render.githubusercontent.com/render/math?math=\alpha_1, ...,\alpha_n" /> and <img src="https://render.githubusercontent.com/render/math?math=\beta_0" /> requires <img src="https://render.githubusercontent.com/render/math?math=n \choose 2" /> inner products <img src="https://render.githubusercontent.com/render/math?math=\langle x_{ij}x_{i^{\prime} j} \rangle" /> between all pairs of training observations.</p><p>Though it might seem like it is necessary to compute the inner product between a new point x and each of the training observations xi to evaluate f(x), as it turns out, αi is zero for all observations except for the support vectors, so only the inner product of the support points need to be calculated.</p><p>Given a collection of the indices of the support points, the linear support classifier can be represented by</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=f(x) = \beta_{0} %2B \sigma_{i \in S}\alpha_{i}\langle x, x_{i}\rangle" /></p><p>which generally requires far less computational resources.</p><p>Going further, it is possible to generalize the support vector classifier by replacing the inner product operations with generalizations of the inner products in the form of</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=K(x_{i}, x_{i^{\prime}})" /></p><p>where <em>K</em> is some function known as a kernel. A kernel is a function that quantifies the similarity of two observations. For example, a kernel of</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=K(x_{i}, x_{i^{\prime}}) = \sum_{j=1}^{p} x_{ij}x_{i^{\prime} j}" /></p><p>would be equivalent to the support vector classifier. This kernel is known as a linear kernel because it is linear in the features. In this case, the linear kernel quantifies the similarity of a pair of observations using Pearson (standard) correlation.</p><p>Another popular kernel function is</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=K(x_{i}, x_{i^{\prime}}) = (1 %2B \sum_{j=1}^{p}x_{ij}x_{i^{\prime}j})^{d}" /></p><p>where <em>d</em> is a positive integer. This kernel is known as a polynomial kernel of degree <em>d</em>. A polynomial kernel leads to a much more flexible decision boundary than using a linear kernel. It essentially amounts to fitting a support vector classifier in a higher dimensional space involving polynomials of degree d, instead of the original feature space.</p><p>When the support vector classifier is combined with a non-linear kernel, the resulting classifier is known as a support vector machine. The non-linear function underlying the support vector machine has the form</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=f(x) = \beta_{0} %2B \sum_{i \in S}\alpha_{i}K(x, x_{i})" /></p><p>Another popular non-linear kernel is the radial kernel given by</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=k(x_{i}, x_{i^{\prime}}) = \mathrm{exp}(-\gamma\sum_{j=1}^{p}(x_{ij} - x_{i^{\prime}j})^{2})" /></p><p>wher γ is a positive constant. The radial kernel has very local behavior, in that only nearby training observations affect the classification of a test observation. This is because if a given test observation, <img src="https://render.githubusercontent.com/render/math?math=x^* = (x^*_1, ..., x^*_p)^T" /> is far from a training observation <img src="https://render.githubusercontent.com/render/math?math=x_i" /> in terms of Euclidean distance, then
<img src="https://render.githubusercontent.com/render/math?math=\sum_{j=1}^{p}(x_{ij} - x_{i^{\prime}j})^{2})" /> will be large and thus <img src="https://render.githubusercontent.com/render/math?math=\mathrm{exp}(-\gamma\sum_{j=1}^{p}(x_{ij} - x_{i^{\prime}j})^{2})" /> will be very tiny and as such xi will have virtually no effect on f(x∗).</p><p>Using a kernel has a computational advantage over simply enlarging the feature space using functions of the original features. Using kernels, it is only necessary to compute <img src="https://render.githubusercontent.com/render/math?math=K(x, x_{i\prime})" /> for <img src="https://render.githubusercontent.com/render/math?math=n \choose 2" />
distinct <em>i</em> and <em>i prime</em>. In some cases, an enlarged feature space can be implicit and of infinite dimension, kernels allow for working solutions in these cases.</p><p>Support vector machines don’t extend well to more than two classes, but two popular approaches for extending SVMs to the K-class case are one-versus-one and one-versus-all.</p><p>Assuming K&gt;2, one-versus-one, or all-pairs, constructs <img src="https://render.githubusercontent.com/render/math?math=K \choose 2" /> SVMs, each of which compares a pair of classes. A test observation would be classified using each of the <img src="https://render.githubusercontent.com/render/math?math=K \choose 2" /> classifiers, with the final classification given by the class most frequently predicted by the <img src="https://render.githubusercontent.com/render/math?math=K \choose 2" /> classifiers.</p><p>Assuming K&gt;2, one-versus-all fits K SVMs, each time comparing one of the K classes to the remaining <em>K−1</em> classes. Assuming a test observation x∗ and coefficients, resulting from fitting an SVM comparing the kth class (coded as +1) to the others (coded as −1), the test observation is assigned to the class for which</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\beta_{0k} + \beta_{1k}X_{1}^{*} +\ \dots\ +\ \beta_{pk}X_{p}^{*}" /></p><p>is largest, as this amounts to the highest level of confidence.</p><p>As a final note, it is important to standardize variables when using the maximal margin classifier, the support vector classifier, and the support vector machine.</p><h2 id="unsupervised-learning">Unsupervised Learning:</h2><p>In the unsupervised learning scenario there are still <em>p</em> predictors and <em>n</em> observations, there is no response variable <em>Y</em>. Instead, the goal is to discover interesting properties of the observations  <img src="https://render.githubusercontent.com/render/math?math=X_1, X_2, ... X_n" />. Two popular unsupervised learning techniques are principal component analysis and clustering.</p><p>Unsupervised learning is often performed as part of exploratory data analysis. Results tend to be more subjective without clear goals and without accepted mechanisms for validating results. In the unsupervised learning scenario there is no correct answer to check predictions against.</p><p><strong>Principal component analysis (PCA)</strong> allows for summarising a large set of correlated variables with a smaller number of representative variables that collectively explain most of the variability in the original set of variables. The principal component directions are the directions of the feature space along which the data are highly variable. These directions also define lines and subspaces that are as close as possible to the data cloud.</p><p>Principal component regression is the result of using principal components as the predictors in a regression model instead of the original set of variables. Principal component analysis refers to the process used to compute the principal components and subsequent use of the components to understand the data.</p><p>Principal component analysis finds a low-dimensional representation of the data set that contains as much of the variation as is possible. Though each observation exists in a p-dimensional space, only a subset of those p-dimensions is interesting, where interesting is measured by the amount that the observations vary along each dimension. Each dimension found by principal component analysis is a linear combination of the <em>p</em> features.</p><p>The first principal component of a set of features is the normalized linear combination of the features:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=Z_{1} = \phi_{11}X_{1} + \phi_{21}X_{2} +\ \dots\ + \phi_{p1}X_{p}" /></p><p>that has the largest variance. Normalisation is achieved by adhering to the constraint</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\sum_{j=1}^{p} \phi_{j1}^{2} = 1" /></p><p>The coefficients here are referred to the loadings of the first principal component, together the loadings make up the principal component loading vector.</p><p>The loadings are constrained such that their sum of squares is equal to one to avoid setting the elements to be arbitrarily large in absolute value which could result in skewing the variance to be arbitrarily large.</p><p>Given an n×p data set <em>X</em>, the first principal component can be computed as follows.</p><p>First, each of the variables in <em>X</em> should be centered to have a mean of zero. Next, an optimization problem is solved that yields the optimal loading vector for the linear combination</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=Z_{ij} = \phi_{11}X_{i1} + \phi_{21}X_{i2} +\ \dots\ + \phi_{p1}X_{ip}" /></p><p>subject to the constraint.</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\sum_{j=1}^{p} \phi_{j1}^{2} = 1" /></p><p>that has the largest sample variance. The optimization problem is defined as</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\mathrm{Maximize}_{\phi_{11},\ \dots,\ \phi_{pi}} \bigg \{\frac{1}{n}\sum_{i=1}^{n}\big(\sum_{j=1}^{p}\phi_{j1}x_{ij}\big)^{2} \bigg \}" /></p><p>subject to</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\sum_{j=1}^{p} \phi_{j1}^{2} = 1" /></p><p>This objective can be restated as</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\frac{1}{n}\sum_{i=1}^{n}z_{i1}^{2}" /></p><p>Also, since variables in <em>x</em> have been centered to have a mean of zero, the average of <img src="https://render.githubusercontent.com/render/math?math=z_{11}, ..., z_{n1}" /> will be zero also. Hence, the objective is to maximize the sample variance of the <em>n</em> values of <img src="https://render.githubusercontent.com/render/math?math=z_{ij}, z_{11}, z_{21},..., z_{n1}" /> are referred to as the scores of the first principal component.</p><p>The given optimisation problem can be solved via an eigendecomposition, a standard technique in linear algebra not covered here. It is worth noting that units are important for principal component analysis, so standardization is often recommended.</p><p>When interpreted in a geometric setting, the first principal component’s loading vector <img src="https://render.githubusercontent.com/render/math?math=\phi_{1}" /> with elements <img src="https://render.githubusercontent.com/render/math?math=\phi_{11}, ..., \phi_{p1}" /> defines a direction in the feature space along which the data vary the most. When the <em>n</em> data points <img src="https://render.githubusercontent.com/render/math?math=x_1, ..., x_{n}" /> are projected onto this direction, the projected values are the principal component scores <img src="https://render.githubusercontent.com/render/math?math=z_{ij}, z_{11},..., z_{n1}" /> themselves.</p><p>As many as Min(n−1,p) principal components can be computed.The second principal component, <img src="https://render.githubusercontent.com/render/math?math=Z_2" /> is the linear combination of <img src="https://render.githubusercontent.com/render/math?math=x_1, ..., x_p" /> that has the maximal variance out of all linear combinations that are uncorrelated with <img src="https://render.githubusercontent.com/render/math?math=Z_1" />. The scores for the second principal component, <img src="https://render.githubusercontent.com/render/math?math=z_{12}, z_{22},..., z_{n2}" />, take the form</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=z_{i2} = \phi_{12}x_{i1} %2B \phi_{22}x_{i2} %2B \dots %2B \phi_{p2}x_{ip}" /></p><p>where <img src="https://render.githubusercontent.com/render/math?math=\phi_{2}" /> is the second principal component loading vector with elements <img src="https://render.githubusercontent.com/render/math?math=\phi_{12}, \phi_{22}, ..., \phi_{p2} " />. It works out that constraining <img src="https://render.githubusercontent.com/render/math?math=Z_{2}" />  to be uncorrelated with <img src="https://render.githubusercontent.com/render/math?math=Z_{1}" /> is equivalent to constraining the direction of <img src="https://render.githubusercontent.com/render/math?math=\phi_{2}" /> to be orthogonal or perpendicular to the direction of <img src="https://render.githubusercontent.com/render/math?math=\phi_{1}" />.</p><p>Another interpretation of principal components is that they provide low-dimensional linear surfaces that are closest to the observations. Under such an interpretation, the first principal component has a very special property: it is the line in p-dimensional space that is closest to the n observations using average squared Euclidean distance as the metric for closeness. This is appealing because a single dimension of the data that lies as close as possible to all of the data points will likely provide a good summary of the data.</p><p>This interpretation extends beyond the first principal component. For example, the first two principal components define the plane that is closest to the n observations in terms of average squared Euclidean distance. Similarly, the first three principal components span the three dimensional hyperplane that is closest to the n observations.</p><p>Under this interpretation, the first M principal component score vectors, combined with the first M principal component loading vectors provide the best M-dimensional approximation (in terms of Euclidean distance) to the ith observation, <img src="https://render.githubusercontent.com/render/math?math=x_{ij}" />. This representation takes the form</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=x_{ij} \approx \sum_{m=1}^{M}Z_{im}\phi_{jm}" /></p><p>assuming the original data matrix, <em>X</em>, is column centered. This means that when M is sufficiently large, the M principal component score vectors and loading vectors can provide a good approximation of the data. When M=Min(n−1, p), the representation is exact:</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=x_{ij} = \sum_{m=1}^{M}Z_{im}\phi_{jm}" /></p><p>The results obtained by performing principal component analysis depend on whether the variables have been individually scaled (each multiplied by a different constant). As such, variables are typically scaled to have a standard deviation of one before performing principal component analysis.</p><p>Principal components are unique and consistent, though signs may vary depending on the calculation method. The portion of variance explained provides a means of determining how much of the variance in the data is not captured by the first M principal components.</p><p>The total variance in the data set assuming the variables have been centered to have a mean of zero is defined by</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\sum_{j=1}^{p}\mathrm{Var}(X_{j}) = \sum_{j=1}^{p}\frac{1}{n}\sum_{i=1}^{n}x_{ij}^{2}" /></p><p>The variance explained by the mth principal component is defined as</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\frac{1}{n}\sum_{i=1}^{n}z_{im}^{2} = \frac{1}{n}\sum_{i=1}^{n}\big(\sum_{j=1}^{p}\phi_{jm}x_{ij}\big)^{2}" /></p><p>From these equations it can be seen that the portion of the variance explained for the mth principal component is given by</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\frac{\sum_{i=1}^{n}\big(\sum_{j=1}^{p}\phi_{jm}x_{ij}\big)^{2}}{\sum_{j=1}^{p}\sum_{i=1}^{n}x_{ij}^{2}}" /></p><p>To compute the cumulative portion of variance explained by the first m principal components, the individual portions should be summed. In total there are Min(n−1, p) principal components and their portion of variance explained sums to one.</p><p>A scree plot can be useful for determining how many principal components is enough, but there is no well accepted, objective way to decide the appropriate number of principal components. Another means is to keep taking principal components while each new principal component explains a sizable portion of the variance. This doesn’t always work well.</p><p>In supervised learning scenarios, cross validation can be used to tune the appropriate number of principal components.</p><p>Applying other statistical learning methods to the principal components instead of the original variables can yield less noisy results.</p><p><strong>Clustering</strong> refers to a broad set of techniques for finding subgroups or clusters in a data set. What constitutes similar or different tends to depend on the domain in question. Clustering can be an unsupervised problem in scenarios where the goal is to discover structure and that structure is not known in advance. Clustering looks for homogeneous subgroups among the observations.</p><p>There are many kinds of clustering. Two of the most popular clustering approaches are k-means clustering and hierarchical clustering. In general, observations are clustered by features in order to identify subgroups among the observations or features can be clustered by observations to try to find subgroups among the features.</p><p><strong>K-means clustering</strong> aims to partition a data set into K distinct, non-overlapping clusters, where K is stipulated in advance.</p><p>The K-means clustering procedure is built on a few constraints. Given sets containing the indices of the observations in each cluster, <img src="https://render.githubusercontent.com/render/math?math=C_1, ..., C_K" />, these sets must satisfy two properties:</p><ol><li><p>Each observation belongs to at least one of the <em>K</em> clusters: <img src="https://render.githubusercontent.com/render/math?math=C_{1} \cup C_{2} \cup \dots \cup C_{K} = \{1,\ \dots,\ n\}" /></p></li><li><p>No observation belongs to more than one cluster. Clusters are non-overlapping: <img src="https://render.githubusercontent.com/render/math?math=C_{k} \cap C_{k^{\prime}} = \{\}\ \mathrm{for\ all\ k,}\ k \neq k^{\prime}" /></p></li></ol><p>In the context of K-means clustering, a good cluster is one for which the within-cluster variation is as small as possible. For a cluster <img src="https://render.githubusercontent.com/render/math?math=C_K" />, the within-cluster variation, <img src="https://render.githubusercontent.com/render/math?math=W(C_K)" />, is a measure of the amount by which the observations in a cluster differ from each other. As such, an ideal cluster would minimize</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\sum_{k=1}^{K}W(C_{k})" /></p><p>Informally, this means that the observations should be partitioned into <em>K</em> clusters such that the total within-cluster variation, summed over all <em>K</em> clusters, is as small as possible.</p><p>In order to solve this optimization problem, it is first necessary to define the means by which within-cluster variation will be evaluated. There are many ways to evaluate within-cluster variation, but the most common choice tends to be squared Euclidean distance, defined as</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=W(C_{k}) = \frac{1}{|C_{k}|}\sum_{i,i^{\prime} \in C_{k}} \sum_{j=1}^{p}(x_{ij} - x_{i^{\prime}j})^{2}" /></p><p>where <img src="https://render.githubusercontent.com/render/math?math=|C_K|" /> denotes the number of observations in the kth cluster.</p><p>Combined with the abstract optimization problem outlined earlier yields</p><p align="center"><img src="https://render.githubusercontent.com/render/math?math=\mathrm{Minimize}_{C_{1},\ \dots,\ C_{K}} \bigg \{\sum_{k=1}^{K}\frac{1}{|C_{k}|}\sum_{i,i^{\prime} \in C_{k}} \sum_{j=1}^{p}(x_{ij} - x_{i^{\prime}j})^{2} \bigg \}" /></p><p>Finding the optimal solution to this problem is computationally infeasible unless <em>K</em> and <em>n</em> are very small, since there are almost <img src="https://render.githubusercontent.com/render/math?math=K^2" /> ways to partition <em>n</em> observations into <em>K</em> clusters. However, a simple algorithm exists to find a local optimum:</p><ol><li>Assign each observation to a cluster from 1 to K.</li><li>Iterate until cluster assignments stop changing:
<ol><li>Compute the cluster centroid for each of the K clusters. The kth cluster centroid is the vector of p feature means for the observations in the kth cluster.</li><li>Assign each observation to the cluster whose centroid is the closest, where closest is defined using Euclidean distance.</li></ol></li></ol><p>Though it may not seem like it, this algorithm is guaranteed to decrease the value of the objective function at each step. This is because the algorithm repeatedly relocates observations based on minimizing the sum of squared deviations, resulting in the sum of squared deviations getting smaller with each iteration. When the result of the algorithm no longer changes, this means a local optimum has been found.</p><p>K-means clustering gets its name from the fact that the cluster centroids are computed as means of the observations assigned to each cluster.</p><p>Because the K-means algorithm finds a local optimum instead of a global optimum, the results obtained will depend on the initial randomized cluster assignments of each observation. For this reason, it is important to run the algorithm multiple times from different initial configurations and select the solution for which the objective is smallest.</p><p><strong>Hierarchical clustering</strong> is an alternative approach to clustering. One disadvantage of K-means clustering is that it requires K to be stated in advance. Hierarchical clustering has the advantage over K-means clustering that it does not require committing to a particular choice of K and in addition, it results in a tree-based representation of the observations called a dendrogram.</p><p>The most common type of hierarchical clustering is bottom-up or agglomerative clustering in which the dendrogram is built starting from the leaves and combining clusters up to the trunk. Based on this, it is not hard to see that the earlier observations or branches fuse together, the more similar they are. Observations that fuse later, near the top of the tree, can be quite different. The height at which two observations fuse together is indicative of how different the observations are.</p><p>No conclusions about similarity should be made from horizontal proximity as this can vary.</p><p>Clusters can be extracted from the dendrogram by making a horizontal cut across the dendrogram and taking the distinct sets of observations below the cut. The height of the cut to the dendrogram serves a similar role to K in K-means clustering: it controls the number of clusters yielded.</p><p>Dendrograms are attractive because a single dendrogram can be used to obtain any number of clusters.</p><p>Often people will look at the dendrogram and select a sensible number of clusters by eye, depending on the heights of the fusions and the number of clusters desired. Unfortunately, the choice of where to cut the dendrogram is not always evident.</p><p>The term hierarchical refers to the fact that the clusters obtained by cutting the dendrogram at the given height are necessarily nested within the clusters obtained by cutting the dendrogram at any greater height.</p><p>The hierarchical structure assumption is not always valid. For example, splitting a group of people in to sexes and splitting a group of people by race yield clusters that aren’t necessarily hierarchical in structure. Because of this, hierarchical clustering can sometimes yield worse results than K-means clustering.</p><p>The hierarchical clustering dendrogram is obtained by first selecting some sort of measure of dissimilarity between the each pair of observations; often Euclidean distance is used. Starting at the bottom of the dendrogram, each of the n observations is treated as its own cluster. With each iteration, the two clusters that are most similar are merged together so there are n−1 clusters. This process is repeated until all the observations belong to a single cluster and the dendrogram is complete.</p><p>The dissimilarity between the two clusters that are merged indicates the height in the dendrogram at which the fusion should be placed.</p><p>One issue not addressed is how clusters with multiple observations are compared. This requires extending the notion of dissimilarity to a pair of groups of observations.</p><p>Linkage defines the dissimilarity between two groups of observations. There are four common types of linkage: complete, average, single, and centroid. Average, complete and single linkage are most popular among statisticians. Centroid linkage is often used in genomics. Average and complete linkage tend to be preferred because they tend to yield more balanced dendrograms. Centroid linkage suffers from a major drawback in that an inversion can occur where two clusters fuse at a height below either of the individual clusters in the dendrogram.</p><p>Complete linkage uses the maximal inter-cluster dissimilarity, calculated by computing all of the pairwise dissimilarities between observations in cluster A and observations in cluster B and taking the largest of those dissimilarities.</p><p>Single linkage uses the minimal inter-cluster dissimilarity given by computing all the pairwise dissimilarities between observations in clusters A and B and taking the smallest of those dissimilarities. Single linkage can result in extended trailing clusters where each observation fuses one-at-a-time.</p><p>Average linkage uses the mean inter-cluster dissimilarity given by computing all pairwise dissimilarities between the observations in cluster A and the observations in cluster B and taking the average of those dissimilarities.</p><p>Centroid linkage computes the dissimilarity between the centroid for cluster and A and the centroid for cluster B.</p><p><strong>Dissimilarity Measures.</strong> Correlation based distance considers two observations to be similar if their features are highly correlated, even though the observed values may be far apart in terms of Euclidean distance. Correlation based distance focuses on the shapes of observation profiles rather than their magnitudes.</p><p>The type of data and the scientific question at hand should determine what dissimilarity measure is used for hierarchical clustering.</p><p>The choice of whether or not to scale the variables before computing the dissimilarity measure depends on the application at hand.</p><p>With K-means clustering and hierarchical clustering, there’s rarely one right answer and a few small decisions can have big consequences:</p><ul><li>Should observations be standardized in some way? For example, observations could be centered to have a mean of zero and/or scaled to have a standard deviation of one.</li><li>In the case of K-means clustering:
<ul><li>How many clusters are desired?</li></ul></li><li>In the case of hierarchical clustering:
<ul><li>What dissimilarity measure should be used?</li><li>What type of linkage should be used?</li><li>At what height should the dendrogram be cut?</li></ul></li></ul><p>There are methods for assigning a p-value to a cluster in order to assess whether there is more evidence for a cluster than would be expected due to chance, however, there’s been no consensus on a single best approach.</p><p>Because clustering can be non-robust to changes in the data set, it’s recommended to cluster subsets of the data to get a sense of the robustness of the yielded clusters.</p><p>Other forms of clustering exist that do not force every observation into a cluster, which can be useful in data sets that contain outliers that do not belong to any cluster. Mixture models can help address such outliers. A soft version of K-means clustering can be effective in these scenarios.</p></im>
</div>

<div id="post-tags">
    <br/>
    <b>Tags: </b>
    
    <a href="/tags-output/statistics/">statistics</a>
    
    <a href="/tags-output/machine%20learning/">machine learning</a>
    
</div>

<br/>


    
    <div id="prev-next">
    
    <div class="prev"></div>
    
    
    <a class="next" href="/posts/2019-09-21-useful-bash-commands/"><div class="nav-text"">Bash: Useful Commands</div><svg class="icon icon-circle-right"><use xlink:href="/img/icons.svg#icon-circle-right"></use></svg></a>
    
</div>

    

    
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        var disqus_config = function () {
            this.page.url = "http://williamgrimes.xyz/posts/2020-05-12-isl-notes/";
            this.page.identifier = "Introduction to Statistical Learning";
        };
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//williamgrimes.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    


</div>

            <hr/>
            <div id="footercont">
                Copyright &copy; 2021 William Grimes
                <br>Powered by <a href="http://cryogenweb.org">Cryogen</a>
            </div>
        </article>

        <script src="/js/highlight.pack.js" type="application/javascript"></script>
        <script>hljs.initHighlightingOnLoad();</script>

        
        
    </body>
</html>
